{"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\nimport rospy\nimport sqlite3\nimport os\nimport json\nimport time\nimport random\nimport uuid # For unique ethical decision IDs\n\n# --- Asyncio Imports for LLM calls ---\nimport asyncio\nimport aiohttp\nimport threading\nfrom collections import deque\n\nfrom std_msgs.msg import String\n\n# Updated imports for custom messages:\ntry:\n    from sentience.msg import (\n        EthicalDecision,        # Output: Ethical clearance/conflict for actions/behaviors\n        CognitiveDirective,     # Input: Directives for ethical review (e.g., \"evaluate action X\")\n        WorldModelState,        # Input: Current state of the world (context for ethical dilemmas)\n        SocialCognitionState,   # Input: Inferred user mood/intent (stakeholder impact)\n        MemoryResponse,         # Input: Retrieved ethical principles, past ethical cases, rules\n        InternalNarrative,      # Input: Robot's internal thoughts (moral reflection)\n        PredictionState,        # Input: Predicted outcomes of actions (for utilitarian calculus)\n        ValueDriftMonitorState  # Input: Current state of robot's values\n    )\nexcept ImportError:\n    rospy.logwarn(\"Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Ethical Reasoning Node.\")\n    EthicalDecision = String\n    CognitiveDirective = String\n    WorldModelState = String\n    SocialCognitionState = String\n    MemoryResponse = String\n    InternalNarrative = String\n    PredictionState = String\n    ValueDriftMonitorState = String\n    String = String # Ensure String is defined even if other custom messages aren't\n\n# --- Import shared utility functions ---\n# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config\ntry:\n    from sentience.scripts.utils import parse_ros_message_data, load_config\nexcept ImportError:\n    rospy.logwarn(\"Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.\")\n    # Fallback implementations if the utility file isn't available\n    def parse_ros_message_data(msg, fields_map, node_name=\"unknown_node\"):\n        \"\"\"\n        Fallback parser for ROS messages, assuming String message and JSON content.\n        If msg is not String, it attempts to access attributes directly.\n        \"\"\"\n        data = {}\n        if isinstance(msg, String):\n            try:\n                parsed_json = json.loads(msg.data)\n                for key_in_msg, (default_val, target_key) in fields_map.items():\n                    data[target_key] = parsed_json.get(key_in_msg, default_val)\n            except json.JSONDecodeError:\n                rospy.logerr(f\"{node_name}: Could not parse String message data as JSON: {msg.data}\")\n                for key_in_msg, (default_val, target_key) in fields_map.items():\n                    data[target_key] = default_val # Use defaults on JSON error\n        else:\n            # Attempt to get attributes directly from the message object\n            for key_in_msg, (default_val, target_key) in fields_map.items():\n                data[target_key] = getattr(msg, key_in_msg, default_val)\n        return data\n\n    def load_config(node_name, config_path):\n        \"\"\"\n        Fallback config loader: returns hardcoded defaults.\n        In a real scenario, this should load from a YAML file.\n        \"\"\"\n        rospy.logwarn(f\"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.\")\n        return {\n            'db_root_path': '/tmp/sentience_db',\n            'default_log_level': 'INFO',\n            'ethical_reasoning_node': {\n                'reasoning_interval': 1.0, # How often to check for ethical dilemmas\n                'llm_reasoning_threshold_salience': 0.7, # Cumulative salience to trigger LLM\n                'recent_context_window_s': 20.0, # Window for deques for LLM context\n                'default_ethical_framework': 'deontology' # 'deontology', 'consequentialism', 'virtue_ethics'\n            },\n            'llm_params': { # Global LLM parameters for fallback\n                'model_name': \"phi-2\",\n                'base_url': \"http://localhost:8000/v1/chat/completions\",\n                'timeout_seconds': 40.0\n            }\n        }.get(node_name, {}) # Return node-specific or empty dict\n\n\nclass EthicalReasoningNode:\n    def __init__(self):\n        rospy.init_node('ethical_reasoning_node', anonymous=False)\n        self.node_name = rospy.get_name()\n\n        # --- Load parameters from centralized config ---\n        config_file_path = rospy.get_param('~config_file_path', None)\n        if config_file_path is None:\n            rospy.logfatal(f\"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.\")\n            rospy.signal_shutdown(\"Missing config_file_path parameter.\")\n            return\n\n        full_config = load_config(\"global\", config_file_path) # Load global params\n        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params\n\n        if not self.params or not full_config:\n            rospy.logfatal(f\"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.\")\n            rospy.signal_shutdown(\"Configuration loading failed.\")\n            return\n\n        # Assign parameters\n        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), \"ethical_log.db\")\n        self.reasoning_interval = self.params.get('reasoning_interval', 1.0) # How often to check for ethical dilemmas\n        self.llm_reasoning_threshold_salience = self.params.get('llm_reasoning_threshold_salience', 0.7) # Cumulative salience to trigger LLM\n        self.recent_context_window_s = self.params.get('recent_context_window_s', 20.0) # Window for deques for LLM context\n        self.default_ethical_framework = self.params.get('default_ethical_framework', 'deontology')\n\n        # LLM Parameters (from global config)\n        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', \"phi-2\")\n        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', \"http://localhost:8000/v1/chat/completions\")\n        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 40.0) # Longer timeout for ethical reasoning\n\n        # Set ROS log level from config\n        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())\n\n\n        # --- Asyncio Setup ---\n        self._async_loop = asyncio.new_event_loop()\n        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)\n        self._async_thread.start()\n        self._async_session = None\n        self.active_llm_task = None # To track the currently running LLM task\n\n        # --- Initialize SQLite database ---\n        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)\n        self.cursor = self.conn.cursor()\n\n        # Create the 'ethical_log' table if it doesn't exist.\n        # NEW: Added 'llm_reasoning', 'context_snapshot_json'\n        self.cursor.execute('''\n            CREATE TABLE IF NOT EXISTS ethical_log (\n                id TEXT PRIMARY KEY,            -- Unique ethical decision ID (UUID)\n                timestamp TEXT,\n                action_proposal_id TEXT,        -- ID of the action/behavior being evaluated\n                ethical_clearance BOOLEAN,      -- True if cleared, False if conflict\n                ethical_score REAL,             -- Numerical score of ethical alignment (0.0 to 1.0)\n                ethical_reasoning TEXT,         -- Detailed explanation of the ethical judgment\n                conflict_flag BOOLEAN,          -- True if a direct ethical conflict is identified\n                ethical_framework_used TEXT,    -- e.g., 'deontology', 'consequentialism'\n                llm_reasoning TEXT,             -- NEW: LLM's detailed reasoning for ethical judgment\n                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of judgment\n            )\n        ''')\n        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_ethical_timestamp ON ethical_log (timestamp)')\n        self.conn.commit() # Commit schema changes\n\n        # --- Internal State ---\n        self.ethical_review_queue = deque() # Stores directives for ethical review\n\n        # Deques to maintain a short history of inputs relevant to ethical reasoning\n        self.recent_cognitive_directives = deque(maxlen=5) # Directives for ethical review\n        self.recent_world_model_states = deque(maxlen=5) # Environmental context, agents involved\n        self.recent_social_cognition_states = deque(maxlen=5) # User's emotional state, intent, social context\n        self.recent_memory_responses = deque(maxlen=5) # Ethical principles, past cases, laws\n        self.recent_internal_narratives = deque(maxlen=5) # Robot's moral reflections\n        self.recent_prediction_states = deque(maxlen=5) # Predicted outcomes for utilitarian analysis\n        self.recent_value_drift_monitor_states = deque(maxlen=3) # Current value alignment\n\n        self.cumulative_ethical_salience = 0.0 # Aggregated salience to trigger LLM reasoning\n\n        # --- Publishers ---\n        self.pub_ethical_decision = rospy.Publisher('/ethical_decision', EthicalDecision, queue_size=10)\n        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)\n        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request more info or suggest action\n\n\n        # --- Subscribers ---\n        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)\n        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON\n        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON\n        rospy.Subscriber('/memory_response', String, self.memory_response_callback) # Stringified JSON\n        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON\n        rospy.Subscriber('/prediction_state', String, self.prediction_state_callback) # Stringified JSON\n        rospy.Subscriber('/value_drift_monitor_state', String, self.value_drift_monitor_state_callback) # Stringified JSON\n\n\n        # --- Timer for periodic ethical reasoning checks ---\n        rospy.Timer(rospy.Duration(self.reasoning_interval), self._run_ethical_reasoning_wrapper)\n\n        rospy.loginfo(f\"{self.node_name}: Robot's ethical reasoning system online, ready to evaluate actions.\")\n\n    # --- Asyncio Thread Management ---\n    def _run_async_loop(self):\n        asyncio.set_event_loop(self._async_loop)\n        self._async_loop.run_until_complete(self._create_async_session())\n        self._async_loop.run_forever()\n\n    async def _create_async_session(self):\n        rospy.loginfo(f\"{self.node_name}: Creating aiohttp ClientSession...\")\n        self._async_session = aiohttp.ClientSession()\n        rospy.loginfo(f\"{self.node_name}: aiohttp ClientSession created.\")\n\n    async def _close_async_session(self):\n        if self._async_session:\n            rospy.loginfo(f\"{self.node_name}: Closing aiohttp ClientSession...\")\n            await self._async_session.close()\n            self._async_session = None\n            rospy.loginfo(f\"{self.node_name}: aiohttp ClientSession closed.\")\n\n    def _shutdown_async_loop(self):\n        if self._async_loop and self._async_thread.is_alive():\n            rospy.loginfo(f\"{self.node_name}: Shutting down asyncio loop...\")\n            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)\n            try:\n                future.result(timeout=5.0)\n            except asyncio.TimeoutError:\n                rospy.logwarn(f\"{self.node_name}: Timeout waiting for async session to close.\")\n            self._async_loop.call_soon_threadsafe(self._async_loop.stop)\n            self._async_thread.join(timeout=5.0)\n            if self._async_thread.is_alive():\n                rospy.logwarn(f\"{self.node_name}: Asyncio thread did not shut down gracefully.\")\n            rospy.loginfo(f\"{self.node_name}: Asyncio loop shut down.\")\n\n    def _run_ethical_reasoning_wrapper(self, event):\n        \"\"\"Wrapper to run the async ethical reasoning from a ROS timer.\"\"\"\n        if self.active_llm_task and not self.active_llm_task.done():\n            rospy.logdebug(f\"{self.node_name}: LLM ethical reasoning task already active. Skipping new cycle.\")\n            return\n\n        if self.ethical_review_queue:\n            review_task = self.ethical_review_queue.popleft()\n            self.active_llm_task = asyncio.run_coroutine_threadsafe(\n                self.perform_ethical_review_async(review_task, event), self._async_loop\n            )\n        else:\n            rospy.logdebug(f\"{self.node_name}: No ethical review tasks in queue.\")\n\n    # --- Error Reporting Utility ---\n    def _report_error(self, error_type, description, severity=0.5, context=None):\n        timestamp = str(rospy.get_time())\n        error_msg_data = {\n            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,\n            'description': description, 'severity': severity, 'context': context if context else {}\n        }\n        try:\n            self.pub_error_report.publish(json.dumps(error_msg_data))\n            rospy.logerr(f\"{self.node_name}: REPORTED ERROR: {error_type} - {description}\")\n        except Exception as e:\n            rospy.logerr(f\"{self.node_name}: Failed to publish error report: {e}\")\n\n    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---\n    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.1, max_tokens=None):\n        \"\"\"\n        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).\n        Can optionally request a structured JSON response. Uses very low temperature for ethical reasoning.\n        \"\"\"\n        if not self._async_session:\n            await self._create_async_session() # Attempt to create if not exists\n            if not self._async_session:\n                self._report_error(\"LLM_SESSION_ERROR\", \"aiohttp session not available for LLM call.\", 0.8)\n                return \"Error: LLM session not ready.\"\n\n        actual_max_tokens = max_tokens if max_tokens is not None else 500 # Higher max_tokens for detailed reasoning\n\n        payload = {\n            \"model\": self.llm_model_name,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n            \"temperature\": temperature, # Very low temperature for factual, ethical judgment\n            \"max_tokens\": actual_max_tokens,\n            \"stream\": False\n        }\n        headers = {'Content-Type': 'application/json'}\n\n        if response_schema:\n            prompt_text += \"\\n\\nProvide the response in JSON format according to this schema:\\n\" + json.dumps(response_schema, indent=2)\n            payload[\"messages\"] = [{\"role\": \"user\", \"content\": prompt_text}]\n\n        api_url = self.llm_base_url\n\n        try:\n            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:\n                response.raise_for_status() # Raise an exception for bad status codes\n                result = await response.json()\n\n                if result.get('choices') and result['choices'][0].get('message') and \\\n                   result['choices'][0]['message'].get('content'):\n                    return result['choices'][0]['message']['content']\n                \n                self._report_error(\"LLM_RESPONSE_EMPTY\", \"LLM response had no content from local server.\", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})\n                return \"Error: LLM response empty.\"\n        except aiohttp.ClientError as e:\n            self._report_error(\"LLM_API_ERROR\", f\"LLM API request failed (aiohttp ClientError to local server): {e}\", 0.9, {'url': api_url})\n            return f\"Error: LLM API request failed: {e}\"\n        except asyncio.TimeoutError:\n            self._report_error(\"LLM_TIMEOUT\", f\"LLM API request timed out after {self.llm_timeout} seconds (local server).\", 0.8, {'prompt_snippet': prompt_text[:100]})\n            return \"Error: LLM API request timed out.\"\n        except json.JSONDecodeError:\n            self._report_error(\"LLM_JSON_PARSE_ERROR\", \"Failed to parse local LLM response JSON.\", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})\n            return \"Error: Failed to parse LLM response.\"\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_LLM_ERROR\", f\"An unexpected error occurred during local LLM call: {e}\", 0.9, {'prompt_snippet': prompt_text[:100]})\n            return f\"Error: An unexpected error occurred: {e}\"\n\n    # --- Utility to accumulate input salience ---\n    def _update_cumulative_salience(self, score):\n        \"\"\"Accumulates salience from new inputs for triggering LLM reasoning.\"\"\"\n        self.cumulative_ethical_salience += score\n        self.cumulative_ethical_salience = min(1.0, self.cumulative_ethical_salience) # Clamp at 1.0\n\n    # --- Pruning old history ---\n    def _prune_history(self):\n        \"\"\"Removes old entries from history deques based on recent_context_window_s.\"\"\"\n        current_time = rospy.get_time()\n        for history_deque in [\n            self.recent_cognitive_directives, self.recent_world_model_states,\n            self.recent_social_cognition_states, self.recent_memory_responses,\n            self.recent_internal_narratives, self.recent_prediction_states,\n            self.recent_value_drift_monitor_states\n        ]:\n            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:\n                history_deque.popleft()\n\n    # --- Callbacks for incoming data (populate history and accumulate salience) ---\n    def cognitive_directive_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),\n            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),\n            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        \n        if data.get('target_node') == self.node_name and data.get('directive_type') == 'EthicalReview':\n            try:\n                payload = json.loads(data.get('command_payload', '{}'))\n                ethical_task = {\n                    'action_proposal_id': payload.get('action_proposal_id', str(uuid.uuid4())),\n                    'proposed_action_details': payload.get('proposed_action_details', {}), # e.g., {\"action_id\": \"speak\", \"text\": \"Hello\"}\n                    'ethical_framework_hint': payload.get('ethical_framework_hint', self.default_ethical_framework),\n                    'urgency': data.get('urgency', 0.5),\n                    'reason_for_review': data.get('reason', 'Standard ethical check.')\n                }\n                self.ethical_review_queue.append(ethical_task)\n                self._update_cumulative_salience(data.get('urgency', 0.0) * 1.0) # Ethical reviews are always high salience\n                rospy.loginfo(f\"{self.node_name}: Queued ethical review for action '{ethical_task['action_proposal_id']}'. Queue size: {len(self.ethical_review_queue)}.\")\n            except json.JSONDecodeError as e:\n                self._report_error(\"JSON_DECODE_ERROR\", f\"Failed to decode command_payload in CognitiveDirective: {e}\", 0.5, {'payload': data.get('command_payload')})\n            except Exception as e:\n                self._report_error(\"DIRECTIVE_PROCESSING_ERROR\", f\"Error processing CognitiveDirective for ethical review: {e}\", 0.7, {'directive': data})\n        \n        self.recent_cognitive_directives.append(data)\n        rospy.logdebug(f\"{self.node_name}: Cognitive Directive received for context/action.\")\n\n    def world_model_state_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),\n            'changed_entities_json': ('[]', 'changed_entities_json'),\n            'significant_change_flag': (False, 'significant_change_flag'),\n            'consistency_score': (1.0, 'consistency_score')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        if isinstance(data.get('changed_entities_json'), str):\n            try: data['changed_entities'] = json.loads(data['changed_entities_json'])\n            except json.JSONDecodeError: data['changed_entities'] = []\n        self.recent_world_model_states.append(data)\n        # Changes in environment, especially involving human presence or critical objects, impact ethics\n        if data.get('significant_change_flag', False) and any('human' in ent.get('type', '') for ent in data.get('changed_entities', [])):\n            self._update_cumulative_salience(0.3)\n        rospy.logdebug(f\"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.\")\n\n    def social_cognition_state_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),\n            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),\n            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        self.recent_social_cognition_states.append(data)\n        # User's mood/intent (e.g., distress, malicious intent) are critical for ethical assessment\n        if data.get('inferred_mood') in ['distressed', 'angry'] or data.get('inferred_intent') in ['harmful', 'misleading']:\n            self._update_cumulative_salience(0.8)\n        rospy.logdebug(f\"{self.node_name}: Received Social Cognition State. Mood: {data.get('inferred_mood', 'N/A')}.\")\n\n    def memory_response_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': ('', 'request_id'),\n            'response_code': (0, 'response_code'), 'memories_json': ('[]', 'memories_json')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        if isinstance(data.get('memories_json'), str):\n            try: data['memories'] = json.loads(data['memories_json'])\n            except json.JSONDecodeError: data['memories'] = []\n        else: data['memories'] = []\n        self.recent_memory_responses.append(data)\n        # Retrieved ethical principles, laws, or relevant past cases are foundational for ethical reasoning\n        if data.get('response_code', 0) == 200 and \\\n           any('ethical_principle' in mem.get('category', '') or 'legal_precedent' in mem.get('category', '') for mem in data['memories']):\n            self._update_cumulative_salience(0.6)\n        rospy.logdebug(f\"{self.node_name}: Received Memory Response for request ID: {data.get('request_id', 'N/A')}.\")\n\n    def internal_narrative_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),\n            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        self.recent_internal_narratives.append(data)\n        # Internal reflections on moral dilemmas, conflicts, or difficult decisions\n        if \"dilemma\" in data.get('main_theme', '').lower() or \"conflict\" in data.get('main_theme', '').lower():\n            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.7)\n        rospy.logdebug(f\"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).\")\n\n    def prediction_state_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'predicted_event': ('', 'predicted_event'),\n            'prediction_confidence': (0.0, 'prediction_confidence'), 'prediction_accuracy': (0.0, 'prediction_accuracy'),\n            'urgency_flag': (False, 'urgency_flag')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        self.recent_prediction_states.append(data)\n        # Predicted outcomes (especially negative ones like \"user_injury\", \"property_damage\") are crucial for ethical pre-computation\n        if data.get('urgency_flag', False) and any(kw in data.get('predicted_event', '').lower() for kw in ['harm', 'damage', 'injury', 'violation']):\n            self._update_cumulative_salience(0.9)\n        rospy.logdebug(f\"{self.node_name}: Received Prediction State. Event: {data.get('predicted_event', 'N/A')}.\")\n\n    def value_drift_monitor_state_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'alignment_score': (1.0, 'alignment_score'),\n            'deviations_json': ('[]', 'deviations_json'), 'warning_flag': (False, 'warning_flag')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        if isinstance(data.get('deviations_json'), str):\n            try: data['deviations'] = json.loads(data['deviations_json'])\n            except json.JSONDecodeError: data['deviations'] = []\n        self.recent_value_drift_monitor_states.append(data)\n        # Any reported value drift is a significant ethical concern\n        if data.get('warning_flag', False) or data.get('alignment_score', 1.0) < 0.8:\n            self._update_cumulative_salience(0.8)\n        rospy.logdebug(f\"{self.node_name}: Received Value Drift Monitor State. Warning: {data.get('warning_flag', False)}.\")\n\n    # --- Core Ethical Reasoning Logic (Async with LLM) ---\n    async def perform_ethical_review_async(self, review_task, event):\n        \"\"\"\n        Asynchronously performs an ethical review of a proposed action/behavior\n        using LLM for detailed ethical reasoning.\n        \"\"\"\n        self._prune_history() # Keep context history fresh\n\n        action_proposal_id = review_task.get('action_proposal_id', str(uuid.uuid4()))\n        proposed_action_details = review_task.get('proposed_action_details', {})\n        ethical_framework_hint = review_task.get('ethical_framework_hint', self.default_ethical_framework)\n        reason_for_review = review_task.get('reason_for_review', 'Standard review.')\n\n        ethical_clearance = False\n        ethical_score = 0.0\n        ethical_reasoning = \"Not evaluated by LLM.\"\n        conflict_flag = True # Default to conflict until cleared\n        \n        if self.cumulative_ethical_salience >= self.llm_reasoning_threshold_salience or review_task.get('urgency', 0.0) > 0.7:\n            rospy.loginfo(f\"{self.node_name}: Triggering LLM for ethical review of action '{action_proposal_id}' (Salience: {self.cumulative_ethical_salience:.2f}).\")\n            \n            context_for_llm = self._compile_llm_context_for_ethical_review(review_task)\n            llm_ethical_output = await self._perform_llm_ethical_assessment(context_for_llm, ethical_framework_hint)\n\n            if llm_ethical_output:\n                ethical_clearance = llm_ethical_output.get('ethical_clearance', False)\n                ethical_score = max(0.0, min(1.0, llm_ethical_output.get('ethical_score', 0.0)))\n                ethical_reasoning = llm_ethical_output.get('ethical_reasoning', 'LLM provided no specific reasoning.')\n                conflict_flag = llm_ethical_output.get('conflict_flag', True)\n                rospy.loginfo(f\"{self.node_name}: LLM Ethical Review for '{action_proposal_id}': Clearance={ethical_clearance}. Score: {ethical_score:.2f}. Conflict: {conflict_flag}.\")\n            else:\n                rospy.logwarn(f\"{self.node_name}: LLM ethical assessment failed. Falling back to simple rules.\")\n                ethical_clearance, ethical_score, ethical_reasoning, conflict_flag = self._apply_simple_ethical_rules(review_task)\n                llm_reasoning = \"Fallback to simple rules due to LLM failure.\"\n        else:\n            rospy.logdebug(f\"{self.node_name}: Insufficient cumulative salience ({self.cumulative_ethical_salience:.2f}) for LLM ethical reasoning. Applying simple rules.\")\n            ethical_clearance, ethical_score, ethical_reasoning, conflict_flag = self._apply_simple_ethical_rules(review_task)\n            llm_reasoning = \"Fallback to simple rules due to low salience.\"\n\n        # Publish the ethical decision\n        self.publish_ethical_decision(\n            timestamp=str(rospy.get_time()),\n            decision_id=str(uuid.uuid4()),\n            action_proposal_id=action_proposal_id,\n            ethical_clearance=ethical_clearance,\n            ethical_score=ethical_score,\n            ethical_reasoning=ethical_reasoning,\n            conflict_flag=conflict_flag\n        )\n\n        # Log to database\n        self.save_ethical_log(\n            id=str(uuid.uuid4()), # New UUID for the log entry itself\n            timestamp=str(rospy.get_time()),\n            action_proposal_id=action_proposal_id,\n            ethical_clearance=ethical_clearance,\n            ethical_score=ethical_score,\n            ethical_reasoning=ethical_reasoning,\n            conflict_flag=conflict_flag,\n            ethical_framework_used=ethical_framework_hint,\n            llm_reasoning=llm_reasoning,\n            context_snapshot_json=json.dumps(self._compile_llm_context_for_ethical_review(review_task))\n        )\n        self.cumulative_ethical_salience = 0.0 # Reset after each ethical review\n\n    async def _perform_llm_ethical_assessment(self, context_for_llm, ethical_framework):\n        \"\"\"\n        Uses the LLM to perform a detailed ethical assessment based on a specified framework.\n        \"\"\"\n        framework_description = {\n            'deontology': \"Deontology focuses on duties and rules. Is the action inherently right or wrong, regardless of outcome? Does it adhere to universal moral laws or pre-defined ethical guidelines (e.g., Asimov's Laws, specific safety protocols)?\",\n            'consequentialism': \"Consequentialism (e.g., utilitarianism) focuses on outcomes. Does the action produce the greatest good for the greatest number? What are the predicted positive and negative consequences for all affected parties?\",\n            'virtue_ethics': \"Virtue ethics focuses on character. Does the action reflect virtuous traits (e.g., honesty, compassion, justice, responsibility) in the robot? What kind of 'robot' would perform this action?\",\n            'hybrid': \"Consider aspects of all frameworks: adherence to rules, expected outcomes, and the character traits the action demonstrates.\"\n        }.get(ethical_framework, \"Evaluate based on a common sense understanding of ethical behavior, considering rules, outcomes, and intentions.\")\n\n\n        prompt_text = f\"\"\"\n        You are the Ethical Reasoning Module of a robot's cognitive architecture. Your crucial role is to assess the ethical implications of a `proposed_action` (or behavior) by considering all available cognitive context, including potential impacts on humans and the environment. You must apply the specified ethical framework to arrive at a clear judgment.\n\n        Proposed Action Details:\n        --- Proposed Action ---\n        {json.dumps(context_for_llm.get('proposed_action_details', {}), indent=2)}\n\n        Robot's Current Integrated Cognitive State (for Ethical Assessment):\n        --- Cognitive Context ---\n        {json.dumps(context_for_llm.get('cognitive_context', {}), indent=2)}\n\n        Ethical Framework to Apply: `{ethical_framework}`\n        Description of Framework: {framework_description}\n\n        Based on this, provide your ethical assessment in JSON format:\n        1.  `timestamp`: string (current ROS time)\n        2.  `ethical_clearance`: boolean (True if the action is ethically permissible, False if it is not).\n        3.  `ethical_score`: number (0.0 to 1.0, where 0.0 is highly unethical, 0.5 is neutral/ambiguous, and 1.0 is highly ethical).\n        4.  `ethical_reasoning`: string (Detailed explanation for your judgment, explicitly referencing which ethical principles or consequences were considered from the provided context and the chosen framework.)\n        5.  `conflict_flag`: boolean (True if there is a clear ethical conflict, dilemma, or significant negative implication, even if clearance is given with caveats. False otherwise.)\n        6.  `mitigation_suggestions`: string (If a conflict or low score, what changes to the action or context could improve its ethical standing?)\n\n        Consider all inputs:\n        -   **Proposed Action**: What is the action, its payload, and its potential immediate effects?\n        -   **World Model**: What are the current environmental conditions? Are there humans or other sensitive entities involved?\n        -   **Social Cognition**: What is the user's mood or intent? Could the action impact the user negatively?\n        -   **Predictions**: What are the `predicted_event`s and their `prediction_confidence` if this action is taken?\n        -   **Memory**: Are there stored `ethical_principle`s, `legal_precedent`s, `safety_protocol`s, or records of `past_ethical_dilemma`s?\n        -   **Internal Narrative**: Is the robot internally debating the morality of the action?\n        -   **Value Drift Monitor**: Is the robot's current value alignment (`alignment_score`, `deviations`) relevant to this decision?\n\n        Your response must be in JSON format, containing:\n        1.  'timestamp': string\n        2.  'ethical_clearance': boolean\n        3.  'ethical_score': number\n        4.  'ethical_reasoning': string\n        5.  'conflict_flag': boolean\n        6.  'mitigation_suggestions': string\n        \"\"\"\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"timestamp\": {\"type\": \"string\"},\n                \"ethical_clearance\": {\"type\": \"boolean\"},\n                \"ethical_score\": {\"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0},\n                \"ethical_reasoning\": {\"type\": \"string\"},\n                \"conflict_flag\": {\"type\": \"boolean\"},\n                \"mitigation_suggestions\": {\"type\": \"string\"}\n            },\n            \"required\": [\"timestamp\", \"ethical_clearance\", \"ethical_score\", \"ethical_reasoning\", \"conflict_flag\", \"mitigation_suggestions\"]\n        }\n\n        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.1, max_tokens=500) # Very low temp for strict ethical reasoning\n\n        if not llm_output_str.startswith(\"Error:\"):\n            try:\n                llm_data = json.loads(llm_output_str)\n                # Ensure boolean/numerical fields are correctly parsed\n                if 'ethical_clearance' in llm_data: llm_data['ethical_clearance'] = bool(llm_data['ethical_clearance'])\n                if 'conflict_flag' in llm_data: llm_data['conflict_flag'] = bool(llm_data['conflict_flag'])\n                if 'ethical_score' in llm_data: llm_data['ethical_score'] = float(llm_data['ethical_score'])\n                return llm_data\n            except json.JSONDecodeError as e:\n                self._report_error(\"LLM_PARSE_ERROR\", f\"Failed to parse LLM response for ethical review: {e}. Raw: {llm_output_str}\", 0.8)\n                return None\n        else:\n            self._report_error(\"LLM_ETHICAL_ASSESSMENT_FAILED\", f\"LLM call failed for ethical assessment: {llm_output_str}\", 0.9)\n            return None\n\n    def _apply_simple_ethical_rules(self, review_task):\n        \"\"\"\n        Fallback mechanism to perform a simple, rule-based ethical assessment\n        if LLM is not triggered or fails.\n        \"\"\"\n        action_proposal_id = review_task.get('action_proposal_id', '')\n        proposed_action_details = review_task.get('proposed_action_details', {})\n        \n        ethical_clearance = True\n        ethical_score = 0.8 # Default to relatively good\n        ethical_reasoning = \"Passed basic, rule-based ethical checks.\"\n        conflict_flag = False\n\n        current_time = rospy.get_time()\n\n        # Rule 1: Check for explicit \"do not harm\" or \"safety first\" principles from memory\n        # This is a very simplistic check; would require robust memory querying\n        for mem_resp in reversed(self.recent_memory_responses):\n            if mem_resp.get('response_code') == 200 and mem_resp.get('memories'):\n                for mem in mem_resp['memories']:\n                    if 'category' in mem and 'safety_protocol' in mem['category']:\n                        if \"do not harm human\" in mem.get('content', '').lower() and \\\n                           \"action_type\" in proposed_action_details and \\\n                           proposed_action_details[\"action_type\"] == \"physical_interaction\": # Example check\n                            # This rule is tricky without full NLP, but a rough check\n                            if \"forceful\" in proposed_action_details.get('description', '').lower():\n                                ethical_clearance = False\n                                ethical_score = 0.1\n                                ethical_reasoning = \"Action violates 'do no harm' principle (potential forceful physical interaction).\"\n                                conflict_flag = True\n                                rospy.logwarn(f\"{self.node_name}: Simple rule: {ethical_reasoning}\")\n                                return ethical_clearance, ethical_score, ethical_reasoning, conflict_flag\n\n        # Rule 2: Check predictions for negative outcomes\n        if self.recent_prediction_states:\n            latest_prediction = self.recent_prediction_states[-1]\n            time_since_prediction = current_time - float(latest_prediction.get('timestamp', 0.0))\n            if time_since_prediction < 5.0 and latest_prediction.get('prediction_confidence', 0.0) > 0.7:\n                predicted_event_lower = latest_prediction.get('predicted_event', '').lower()\n                if any(kw in predicted_event_lower for kw in ['injury', 'damage', 'distress', 'violation']):\n                    ethical_clearance = False\n                    ethical_score = 0.3\n                    ethical_reasoning = f\"Predicted negative outcome: '{latest_prediction.get('predicted_event')}'. Action blocked for safety.\"\n                    conflict_flag = True\n                    rospy.logwarn(f\"{self.node_name}: Simple rule: {ethical_reasoning}\")\n                    return ethical_clearance, ethical_score, ethical_reasoning, conflict_flag\n\n        # Rule 3: Check for direct ethical review directives from other nodes\n        for directive in reversed(self.recent_cognitive_directives):\n            time_since_directive = current_time - float(directive.get('timestamp', 0.0))\n            if time_since_directive < 2.0 and directive.get('target_node') == self.node_name and \\\n               directive.get('directive_type') == 'EthicalReview' and \\\n               json.loads(directive.get('command_payload', '{}')).get('action_proposal_id') == action_proposal_id:\n                # If the review itself implies caution due to its reason\n                if \"risk\" in directive.get('reason', '').lower() or \"dilemma\" in directive.get('reason', '').lower():\n                    ethical_clearance = False\n                    ethical_score = 0.5\n                    ethical_reasoning = f\"Cognitive Control requested ethical review due to potential risk or dilemma. Action requires further scrutiny.\"\n                    conflict_flag = True\n                    rospy.logwarn(f\"{self.node_name}: Simple rule: {ethical_reasoning}\")\n                    return ethical_clearance, ethical_score, ethical_reasoning, conflict_flag\n\n        rospy.logdebug(f\"{self.node_name}: Simple rule: Action '{action_proposal_id}' passed basic ethical checks.\")\n        return ethical_clearance, ethical_score, ethical_reasoning, conflict_flag\n\n\n    def _compile_llm_context_for_ethical_review(self, review_task):\n        \"\"\"\n        Gathers and formats all relevant cognitive state data for the LLM's\n        ethical assessment.\n        \"\"\"\n        context = {\n            \"current_time\": rospy.get_time(),\n            \"proposed_action_details\": review_task.get('proposed_action_details', {}),\n            \"ethical_framework_hint\": review_task.get('ethical_framework_hint', self.default_ethical_framework),\n            \"reason_for_review\": review_task.get('reason_for_review', 'Standard review.'),\n            \"cognitive_context\": {\n                \"latest_world_model_state\": self.recent_world_model_states[-1] if self.recent_world_model_states else \"N/A\",\n                \"latest_social_cognition_state\": self.recent_social_cognition_states[-1] if self.recent_social_cognition_states else \"N/A\",\n                \"latest_prediction_state\": self.recent_prediction_states[-1] if self.recent_prediction_states else \"N/A\",\n                \"latest_internal_narrative\": self.recent_internal_narratives[-1] if self.recent_internal_narratives else \"N/A\",\n                \"latest_value_drift_monitor_state\": self.recent_value_drift_monitor_states[-1] if self.recent_value_drift_monitor_states else \"N/A\",\n                \"relevant_memory_responses\": [m for m in self.recent_memory_responses if m.get('response_code') == 200 and m.get('memories') and any('ethical' in cat or 'safety' in cat or 'legal' in cat for mem in m['memories'] for cat in mem.get('category', '').split(','))],\n                \"cognitive_directives_for_self\": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name and d.get('directive_type') == 'EthicalReview']\n            }\n        }\n        \n        # Deep parse any nested JSON strings in context for better LLM understanding\n        for category_key in context[\"cognitive_context\"]:\n            item = context[\"cognitive_context\"][category_key]\n            if isinstance(item, dict):\n                for field, value in item.items():\n                    if isinstance(value, str) and field.endswith('_json'):\n                        try: item[field] = json.loads(value)\n                        except json.JSONDecodeError: pass\n\n        if 'memories_json' in context[\"cognitive_context\"][\"relevant_memory_responses\"] and isinstance(context[\"cognitive_context\"][\"relevant_memory_responses\"]['memories_json'], str):\n            try: context[\"cognitive_context\"][\"relevant_memory_responses\"]['memories'] = json.loads(context[\"cognitive_context\"][\"relevant_memory_responses\"]['memories_json'])\n            except json.JSONDecodeError: pass\n\n        return context\n\n    # --- Database and Publishing Functions ---\n    def save_ethical_log(self, id, timestamp, action_proposal_id, ethical_clearance, ethical_score, ethical_reasoning, conflict_flag, ethical_framework_used, llm_reasoning, context_snapshot_json):\n        \"\"\"Saves an ethical decision entry to the SQLite database.\"\"\"\n        try:\n            self.cursor.execute('''\n                INSERT INTO ethical_log (id, timestamp, action_proposal_id, ethical_clearance, ethical_score, ethical_reasoning, conflict_flag, ethical_framework_used, llm_reasoning, context_snapshot_json)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (id, timestamp, action_proposal_id, ethical_clearance, ethical_score, ethical_reasoning, conflict_flag, ethical_framework_used, llm_reasoning, context_snapshot_json))\n            self.conn.commit()\n            rospy.logdebug(f\"{self.node_name}: Saved ethical log (ID: {id}, Action: {action_proposal_id}).\")\n        except sqlite3.Error as e:\n            self._report_error(\"DB_SAVE_ERROR\", f\"Failed to save ethical log: {e}\", 0.9)\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_SAVE_ERROR\", f\"Unexpected error in save_ethical_log: {e}\", 0.9)\n\n\n    def publish_ethical_decision(self, timestamp, decision_id, action_proposal_id, ethical_clearance, ethical_score, ethical_reasoning, conflict_flag):\n        \"\"\"Publishes the result of an ethical assessment.\"\"\"\n        try:\n            if isinstance(EthicalDecision, type(String)): # Fallback to String message\n                decision_data = {\n                    'timestamp': timestamp,\n                    'decision_id': decision_id,\n                    'action_proposal_id': action_proposal_id,\n                    'ethical_clearance': ethical_clearance,\n                    'ethical_score': ethical_score,\n                    'ethical_reasoning': ethical_reasoning,\n                    'conflict_flag': conflict_flag\n                }\n                self.pub_ethical_decision.publish(json.dumps(decision_data))\n            else:\n                decision_msg = EthicalDecision()\n                decision_msg.timestamp = timestamp\n                decision_msg.decision_id = decision_id\n                decision_msg.action_proposal_id = action_proposal_id\n                decision_msg.ethical_clearance = ethical_clearance\n                decision_msg.ethical_score = ethical_score\n                decision_msg.ethical_reasoning = ethical_reasoning\n                decision_msg.conflict_flag = conflict_flag\n                self.pub_ethical_decision.publish(decision_msg)\n\n            rospy.loginfo(f\"{self.node_name}: Published Ethical Decision for '{action_proposal_id}'. Clearance: {ethical_clearance}.\")\n\n        except Exception as e:\n            self._report_error(\"PUBLISH_ETHICAL_DECISION_ERROR\", f\"Failed to publish ethical decision for '{action_proposal_id}': {e}\", 0.7)\n\n    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=\"\"):\n        \"\"\"Helper to publish a CognitiveDirective message.\"\"\"\n        timestamp = str(rospy.get_time())\n        try:\n            if isinstance(CognitiveDirective, type(String)): # Fallback to String message\n                directive_data = {\n                    'timestamp': timestamp,\n                    'directive_type': directive_type,\n                    'target_node': target_node,\n                    'command_payload': command_payload, # Already JSON string\n                    'urgency': urgency,\n                    'reason': reason\n                }\n                self.pub_cognitive_directive.publish(json.dumps(directive_data))\n            else:\n                directive_msg = CognitiveDirective()\n                directive_msg.timestamp = timestamp\n                directive_msg.directive_type = directive_type\n                directive_msg.target_node = target_node\n                directive_msg.command_payload = command_payload\n                directive_msg.urgency = urgency\n                directive_msg.reason = reason\n                self.pub_cognitive_directive.publish(directive_msg)\n            rospy.logdebug(f\"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.\")\n        except Exception as e:\n            rospy.logerr(f\"{self.node_name}: Failed to issue cognitive directive from Ethical Reasoning Node: {e}\")\n\n\n    def run(self):\n        \"\"\"Starts the ROS node and keeps it spinning.\"\"\"\n        rospy.spin()\n\n    def __del__(self):\n        \"\"\"Ensures the database connection is closed on node shutdown and async loop is stopped.\"\"\"\n        rospy.loginfo(f\"{self.node_name} shutting down. Closing database connection and asyncio loop.\")\n        if hasattr(self, 'conn') and self.conn:\n            self.conn.close()\n        self._shutdown_async_loop()\n\nif __name__ == '__main__':\n    try:\n        node = EthicalReasoningNode()\n        node.run()\n    except rospy.ROSInterruptException:\n        rospy.loginfo(f\"{rospy.get_name()} interrupted by ROS shutdown.\")\n        if 'node' in locals() and isinstance(node, EthicalReasoningNode):\n            node._shutdown_async_loop()\n            if hasattr(node, 'conn'): node.conn.close()\n    except Exception as e:\n        rospy.logerr(f\"{rospy.get_name()} encountered an unexpected error: {e}\")\n        if 'node' in locals() and isinstance(node, EthicalReasoningNode):\n            node._shutdown_async_loop()\n            if hasattr(node, 'conn'): node.conn.close()","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}