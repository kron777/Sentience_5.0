{"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\nimport rospy\nimport sqlite3\nimport os\nimport json\nimport time\nimport random\nimport uuid # For unique memory IDs and request IDs\n\n# --- Asyncio Imports for LLM calls ---\nimport asyncio\nimport aiohttp\nimport threading\nfrom collections import deque\n\nfrom std_msgs.msg import String\n\n# Updated imports for custom messages:\ntry:\n    from sentience.msg import (\n        MemoryRequest,          # Input: Request to store or retrieve memories\n        MemoryResponse,         # Output: Response to a memory request\n        CognitiveDirective,     # Input: Directives for memory management (e.g., \"forget X\", \"summarize Y\")\n        InternalNarrative,      # Input: Robot's internal thoughts (can be stored as narrative memory)\n        WorldModelState,        # Input: World state updates (can be stored as episodic memory)\n        SocialCognitionState    # Input: Social interactions (can be stored as social memory)\n    )\nexcept ImportError:\n    rospy.logwarn(\"Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Memory Node.\")\n    MemoryRequest = String\n    MemoryResponse = String\n    CognitiveDirective = String\n    InternalNarrative = String\n    WorldModelState = String\n    SocialCognitionState = String\n    String = String # Ensure String is defined even if other custom messages aren't\n\n# --- Import shared utility functions ---\n# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config\ntry:\n    from sentience.scripts.utils import parse_ros_message_data, load_config\nexcept ImportError:\n    rospy.logwarn(\"Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.\")\n    # Fallback implementations if the utility file isn't available\n    def parse_ros_message_data(msg, fields_map, node_name=\"unknown_node\"):\n        \"\"\"\n        Fallback parser for ROS messages, assuming String message and JSON content.\n        If msg is not String, it attempts to access attributes directly.\n        \"\"\"\n        data = {}\n        if isinstance(msg, String):\n            try:\n                parsed_json = json.loads(msg.data)\n                for key_in_msg, (default_val, target_key) in fields_map.items():\n                    data[target_key] = parsed_json.get(key_in_msg, default_val)\n            except json.JSONDecodeError:\n                rospy.logerr(f\"{node_name}: Could not parse String message data as JSON: {msg.data}\")\n                for key_in_msg, (default_val, target_key) in fields_map.items():\n                    data[target_key] = default_val # Use defaults on JSON error\n        else:\n            # Attempt to get attributes directly from the message object\n            for key_in_msg, (default_val, target_key) in fields_map.items():\n                data[target_key] = getattr(msg, key_in_msg, default_val)\n        return data\n\n    def load_config(node_name, config_path):\n        \"\"\"\n        Fallback config loader: returns hardcoded defaults.\n        In a real scenario, this should load from a YAML file.\n        \"\"\"\n        rospy.logwarn(f\"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.\")\n        return {\n            'db_root_path': '/tmp/sentience_db',\n            'default_log_level': 'INFO',\n            'memory_node': {\n                'memory_processing_interval': 0.1, # How often to check memory queue\n                'llm_memory_threshold_salience': 0.7, # Cumulative salience to trigger LLM\n                'recent_context_window_s': 30.0 # Context window for LLM for memory tasks\n            },\n            'llm_params': { # Global LLM parameters for fallback\n                'model_name': \"phi-2\",\n                'base_url': \"http://localhost:8000/v1/chat/completions\",\n                'timeout_seconds': 45.0\n            }\n        }.get(node_name, {}) # Return node-specific or empty dict\n\n\nclass MemoryNode:\n    def __init__(self):\n        rospy.init_node('memory_node', anonymous=False)\n        self.node_name = rospy.get_name()\n\n        # --- Load parameters from centralized config ---\n        config_file_path = rospy.get_param('~config_file_path', None)\n        if config_file_path is None:\n            rospy.logfatal(f\"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.\")\n            rospy.signal_shutdown(\"Missing config_file_path parameter.\")\n            return\n\n        full_config = load_config(\"global\", config_file_path) # Load global params\n        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params\n\n        if not self.params or not full_config:\n            rospy.logfatal(f\"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.\")\n            rospy.signal_shutdown(\"Configuration loading failed.\")\n            return\n\n        # Assign parameters\n        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), \"long_term_memory.db\")\n        self.memory_processing_interval = self.params.get('memory_processing_interval', 0.1) # How often to process requests\n        self.llm_memory_threshold_salience = self.params.get('llm_memory_threshold_salience', 0.7) # Salience to trigger LLM\n        self.recent_context_window_s = self.params.get('recent_context_window_s', 30.0) # Context window for LLM\n\n        # LLM Parameters (from global config)\n        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', \"phi-2\")\n        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', \"http://localhost:8000/v1/chat/completions\")\n        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 45.0) # Longer timeout for memory queries\n\n        # Set ROS log level from config\n        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())\n\n\n        # --- Asyncio Setup ---\n        self._async_loop = asyncio.new_event_loop()\n        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)\n        self._async_thread.start()\n        self._async_session = None\n        self.active_llm_task = None # To track the currently running LLM task\n\n        # --- Initialize SQLite database ---\n        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)\n        self.cursor = self.conn.cursor()\n\n        # Create the 'memories' table if it doesn't exist.\n        # NEW: Added 'llm_processing_notes', 'original_context_json'\n        self.cursor.execute('''\n            CREATE TABLE IF NOT EXISTS memories (\n                id TEXT PRIMARY KEY,            -- Unique memory ID (UUID)\n                timestamp TEXT,\n                category TEXT,                  -- e.g., 'episodic', 'semantic', 'procedural', 'narrative', 'social', 'fact'\n                content TEXT,                   -- The actual memory content (text or JSON string of complex data)\n                keywords TEXT,                  -- Comma-separated keywords for faster retrieval\n                source_node TEXT,               -- Which node created/requested storage of this memory\n                salience REAL,                  -- How important or vivid the memory is (0.0 to 1.0)\n                llm_processing_notes TEXT,      -- NEW: LLM's notes on processing/summarizing the memory\n                original_context_json TEXT      -- NEW: JSON of the original cognitive context when memory was created\n            )\n        ''')\n        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_timestamp ON memories (timestamp)')\n        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_category ON memories (category)')\n        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_memories_keywords ON memories (keywords)')\n        self.conn.commit() # Commit schema changes\n\n        # --- Internal State ---\n        self.memory_request_queue = deque() # Stores incoming memory requests\n\n        # Deques to maintain a short history of inputs for general context, not direct storage\n        self.recent_cognitive_directives = deque(maxlen=5) # Directives for self (e.g., \"summarize memory type X\")\n        self.recent_internal_narratives = deque(maxlen=5) # Can be candidate memories for storage\n        self.recent_world_model_states = deque(maxlen=5) # Can be candidate memories for storage\n        self.recent_social_cognition_states = deque(maxlen=5) # Can be candidate memories for storage\n\n        self.cumulative_memory_salience = 0.0 # Aggregated salience to trigger LLM operations\n\n        # --- Publishers ---\n        self.pub_memory_response = rospy.Publisher('/memory_response', MemoryResponse, queue_size=10)\n        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)\n        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # For requesting attention or other nodes\n\n\n        # --- Subscribers ---\n        rospy.Subscriber('/memory_request', MemoryRequest, self.memory_request_callback)\n        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)\n        rospy.Subscriber('/internal_narrative', InternalNarrative, self.internal_narrative_callback) # Stringified JSON\n        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON\n        rospy.Subscriber('/social_cognition_state', String, self.social_cognition_state_callback) # Stringified JSON\n\n\n        # --- Timer for periodic memory processing ---\n        rospy.Timer(rospy.Duration(self.memory_processing_interval), self._run_memory_processing_wrapper)\n\n        rospy.loginfo(f\"{self.node_name}: Robot's memory system online, database at {self.db_path}.\")\n\n    # --- Asyncio Thread Management ---\n    def _run_async_loop(self):\n        asyncio.set_event_loop(self._async_loop)\n        self._async_loop.run_until_complete(self._create_async_session())\n        self._async_loop.run_forever()\n\n    async def _create_async_session(self):\n        rospy.loginfo(f\"{self.node_name}: Creating aiohttp ClientSession...\")\n        self._async_session = aiohttp.ClientSession()\n        rospy.loginfo(f\"{self.node_name}: aiohttp ClientSession created.\")\n\n    async def _close_async_session(self):\n        if self._async_session:\n            rospy.loginfo(f\"{self.node_name}: Closing aiohttp ClientSession...\")\n            await self._async_session.close()\n            self._async_session = None\n            rospy.loginfo(f\"{self.node_name}: aiohttp ClientSession closed.\")\n\n    def _shutdown_async_loop(self):\n        if self._async_loop and self._async_thread.is_alive():\n            rospy.loginfo(f\"{self.node_name}: Shutting down asyncio loop...\")\n            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)\n            try:\n                future.result(timeout=5.0)\n            except asyncio.TimeoutError:\n                rospy.logwarn(f\"{self.node_name}: Timeout waiting for async session to close.\")\n            self._async_loop.call_soon_threadsafe(self._async_loop.stop)\n            self._async_thread.join(timeout=5.0)\n            if self._async_thread.is_alive():\n                rospy.logwarn(f\"{self.node_name}: Asyncio thread did not shut down gracefully.\")\n            rospy.loginfo(f\"{self.node_name}: Asyncio loop shut down.\")\n\n    def _run_memory_processing_wrapper(self, event):\n        \"\"\"Wrapper to run the async memory processing from a ROS timer.\"\"\"\n        if self.active_llm_task and not self.active_llm_task.done():\n            rospy.logdebug(f\"{self.node_name}: LLM memory processing task already active. Skipping new cycle.\")\n            return\n\n        if self.memory_request_queue:\n            request_data = self.memory_request_queue.popleft()\n            self.active_llm_task = asyncio.run_coroutine_threadsafe(\n                self.process_memory_request_async(request_data, event), self._async_loop\n            )\n        else:\n            rospy.logdebug(f\"{self.node_name}: No memory requests in queue.\")\n\n    # --- Error Reporting Utility ---\n    def _report_error(self, error_type, description, severity=0.5, context=None):\n        timestamp = str(rospy.get_time())\n        error_msg_data = {\n            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,\n            'description': description, 'severity': severity, 'context': context if context else {}\n        }\n        try:\n            self.pub_error_report.publish(json.dumps(error_msg_data))\n            rospy.logerr(f\"{self.node_name}: REPORTED ERROR: {error_type} - {description}\")\n        except Exception as e:\n            rospy.logerr(f\"{self.node_name}: Failed to publish error report: {e}\")\n\n    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---\n    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.2, max_tokens=None):\n        \"\"\"\n        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).\n        Can optionally request a structured JSON response. Low temperature for factual/summarization.\n        \"\"\"\n        if not self._async_session:\n            await self._create_async_session() # Attempt to create if not exists\n            if not self._async_session:\n                self._report_error(\"LLM_SESSION_ERROR\", \"aiohttp session not available for LLM call.\", 0.8)\n                return \"Error: LLM session not ready.\"\n\n        actual_max_tokens = max_tokens if max_tokens is not None else 600 # Higher max_tokens for summarization\n\n        payload = {\n            \"model\": self.llm_model_name,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n            \"temperature\": temperature, # Low temperature for factual consistency\n            \"max_tokens\": actual_max_tokens,\n            \"stream\": False\n        }\n        headers = {'Content-Type': 'application/json'}\n\n        if response_schema:\n            prompt_text += \"\\n\\nProvide the response in JSON format according to this schema:\\n\" + json.dumps(response_schema, indent=2)\n            payload[\"messages\"] = [{\"role\": \"user\", \"content\": prompt_text}]\n\n        api_url = self.llm_base_url\n\n        try:\n            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:\n                response.raise_for_status() # Raise an exception for bad status codes\n                result = await response.json()\n\n                if result.get('choices') and result['choices'][0].get('message') and \\\n                   result['choices'][0]['message'].get('content'):\n                    return result['choices'][0]['message']['content']\n                \n                self._report_error(\"LLM_RESPONSE_EMPTY\", \"LLM response had no content from local server.\", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})\n                return \"Error: LLM response empty.\"\n        except aiohttp.ClientError as e:\n            self._report_error(\"LLM_API_ERROR\", f\"LLM API request failed (aiohttp ClientError to local server): {e}\", 0.9, {'url': api_url})\n            return f\"Error: LLM API request failed: {e}\"\n        except asyncio.TimeoutError:\n            self._report_error(\"LLM_TIMEOUT\", f\"LLM API request timed out after {self.llm_timeout} seconds (local server).\", 0.8, {'prompt_snippet': prompt_text[:100]})\n            return \"Error: LLM API request timed out.\"\n        except json.JSONDecodeError:\n            self._report_error(\"LLM_JSON_PARSE_ERROR\", \"Failed to parse local LLM response JSON.\", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})\n            return \"Error: Failed to parse LLM response.\"\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_LLM_ERROR\", f\"An unexpected error occurred during local LLM call: {e}\", 0.9, {'prompt_snippet': prompt_text[:100]})\n            return f\"Error: An unexpected error occurred: {e}\"\n\n    # --- Utility to accumulate input salience ---\n    def _update_cumulative_salience(self, score):\n        \"\"\"Accumulates salience from new inputs for triggering LLM operations.\"\"\"\n        self.cumulative_memory_salience += score\n        self.cumulative_memory_salience = min(1.0, self.cumulative_memory_salience) # Clamp at 1.0\n\n    # --- Pruning old history ---\n    def _prune_history(self):\n        \"\"\"Removes old entries from history deques based on recent_context_window_s.\"\"\"\n        current_time = rospy.get_time()\n        for history_deque in [\n            self.recent_cognitive_directives, self.recent_internal_narratives,\n            self.recent_world_model_states, self.recent_social_cognition_states\n        ]:\n            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:\n                history_deque.popleft()\n\n    # --- Callbacks for incoming data (populate history) ---\n    def memory_request_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'request_id': (str(uuid.uuid4()), 'request_id'),\n            'request_type': ('retrieve', 'request_type'), # 'store', 'retrieve', 'update', 'delete', 'summarize'\n            'category': ('general', 'category'),\n            'query_text': ('', 'query_text'),\n            'content_json': ('{}', 'content_json'), # For 'store' or 'update' requests\n            'keywords': ('', 'keywords'),\n            'num_results': (1, 'num_results'), # For 'retrieve' requests\n            'salience': (0.5, 'salience'), # For 'store' requests, importance of the memory\n            'source_node': (self.node_name, 'source_node')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        \n        # Parse content_json if it's a string\n        if isinstance(data.get('content_json'), str):\n            try:\n                data['content_parsed'] = json.loads(data['content_json'])\n            except json.JSONDecodeError:\n                data['content_parsed'] = {} # Fallback if not valid JSON\n        else:\n            data['content_parsed'] = data.get('content_json', {}) # Ensure it's a dict\n\n        self.memory_request_queue.append(data)\n        # Salience of the request directly influences LLM trigger\n        self._update_cumulative_salience(data.get('salience', 0.5) * 0.7) # Memory requests are usually important\n        rospy.loginfo(f\"{self.node_name}: Queued memory request (ID: {data['request_id']}, Type: {data['request_type']}). Queue size: {len(self.memory_request_queue)}.\")\n\n    def cognitive_directive_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),\n            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),\n            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        \n        if data.get('target_node') == self.node_name:\n            self.recent_cognitive_directives.append(data) # Add directives for self to context\n            # Directives for memory management (e.g., 'OptimizeMemory', 'ForgetMemory') are highly salient\n            if data.get('directive_type') in ['OptimizeMemory', 'ForgetMemory', 'SummarizeMemory']:\n                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9)\n            rospy.loginfo(f\"{self.node_name}: Received directive for self: '{data.get('directive_type', 'N/A')}' (Payload: {data.get('command_payload', 'N/A')}).\")\n        else:\n            self.recent_cognitive_directives.append(data) # Add all directives for general context\n        rospy.logdebug(f\"{self.node_name}: Cognitive Directive received for context/action.\")\n\n    def internal_narrative_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'narrative_text': ('', 'narrative_text'),\n            'main_theme': ('', 'main_theme'), 'sentiment': (0.0, 'sentiment'), 'salience_score': (0.0, 'salience_score')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        self.recent_internal_narratives.append(data)\n        # Internal narratives can be good candidates for narrative memory storage\n        if data.get('salience_score', 0.0) > 0.6:\n            # Auto-store high salience internal narratives as memories\n            self.memory_request_queue.append({\n                'timestamp': data['timestamp'],\n                'request_id': str(uuid.uuid4()),\n                'request_type': 'store',\n                'category': 'narrative',\n                'query_text': '',\n                'content_json': json.dumps({'text': data['narrative_text'], 'theme': data['main_theme'], 'sentiment': data['sentiment']}),\n                'keywords': data['main_theme'].replace('_', ','),\n                'num_results': 0,\n                'salience': data['salience_score'],\n                'source_node': 'internal_narrative_node'\n            })\n            self._update_cumulative_salience(data.get('salience_score', 0.0) * 0.3)\n        rospy.logdebug(f\"{self.node_name}: Received Internal Narrative (Theme: {data.get('main_theme', 'N/A')}).\")\n\n    def world_model_state_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),\n            'changed_entities_json': ('[]', 'changed_entities_json'),\n            'significant_change_flag': (False, 'significant_change_flag'),\n            'consistency_score': (1.0, 'consistency_score')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        if isinstance(data.get('changed_entities_json'), str):\n            try: data['changed_entities'] = json.loads(data['changed_entities_json'])\n            except json.JSONDecodeError: data['changed_entities'] = []\n        self.recent_world_model_states.append(data)\n        # Significant world model changes can be stored as episodic memories\n        if data.get('significant_change_flag', False):\n            # Auto-store significant world model changes as episodic memories\n            self.memory_request_queue.append({\n                'timestamp': data['timestamp'],\n                'request_id': str(uuid.uuid4()),\n                'request_type': 'store',\n                'category': 'episodic',\n                'query_text': '',\n                'content_json': json.dumps({'description': f\"Significant change in world state: {len(data['changed_entities'])} entities changed.\", 'details': data}),\n                'keywords': \"world_change,environment\",\n                'num_results': 0,\n                'salience': 0.7, # High salience for important world changes\n                'source_node': 'world_model_node'\n            })\n            self._update_cumulative_salience(0.4)\n        rospy.logdebug(f\"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.\")\n\n    def social_cognition_state_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'inferred_mood': ('neutral', 'inferred_mood'),\n            'mood_confidence': (0.0, 'mood_confidence'), 'inferred_intent': ('none', 'inferred_intent'),\n            'intent_confidence': (0.0, 'intent_confidence'), 'user_id': ('unknown', 'user_id')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        self.recent_social_cognition_states.append(data)\n        # Important social interactions can be stored as social memories\n        if data.get('intent_confidence', 0.0) > 0.6 or data.get('mood_confidence', 0.0) > 0.6:\n            # Auto-store salient social interactions as social memories\n            self.memory_request_queue.append({\n                'timestamp': data['timestamp'],\n                'request_id': str(uuid.uuid4()),\n                'request_type': 'store',\n                'category': 'social',\n                'query_text': '',\n                'content_json': json.dumps({'user_id': data['user_id'], 'inferred_mood': data['inferred_mood'], 'inferred_intent': data['inferred_intent']}),\n                'keywords': f\"social,user_{data['user_id']}\",\n                'num_results': 0,\n                'salience': data.get('intent_confidence', 0.0) * 0.5 + data.get('mood_confidence', 0.0) * 0.5,\n                'source_node': 'social_cognition_node'\n            })\n            self._update_cumulative_salience(0.3)\n        rospy.logdebug(f\"{self.node_name}: Received Social Cognition State. Intent: {data.get('inferred_intent', 'N/A')}.\")\n\n    # --- Core Memory Processing Logic (Async with LLM) ---\n    async def process_memory_request_async(self, request_data, event):\n        \"\"\"\n        Asynchronously processes a memory request (store, retrieve, update, delete, summarize)\n        using LLM for advanced operations.\n        \"\"\"\n        self._prune_history() # Keep context history fresh\n\n        request_id = request_data.get('request_id')\n        request_type = request_data.get('request_type')\n        category = request_data.get('category')\n        query_text = request_data.get('query_text')\n        content_json = request_data.get('content_json') # Original JSON string\n        content_parsed = request_data.get('content_parsed') # Already parsed dict\n        keywords = request_data.get('keywords')\n        num_results = request_data.get('num_results', 1)\n        salience = request_data.get('salience', 0.5)\n        source_node = request_data.get('source_node', 'unknown')\n\n        memory_response_code = 500 # Default to error\n        retrieved_memories = []\n        llm_processing_notes = \"No LLM processing.\"\n        original_context_snapshot = self._compile_llm_context_for_memory_op(request_data)\n\n        if request_type == 'store':\n            # LLM can process content to extract better keywords or summarize\n            if self.cumulative_memory_salience >= self.llm_memory_threshold_salience and salience >= 0.6:\n                rospy.loginfo(f\"{self.node_name}: Triggering LLM for memory store processing (ID: {request_id}, Category: {category}).\")\n                llm_processed_memory = await self._process_memory_content_llm(content_parsed, category, keywords, original_context_snapshot)\n                if llm_processed_memory:\n                    content_to_store = llm_processed_memory.get('processed_content', content_parsed)\n                    keywords_to_store = llm_processed_memory.get('extracted_keywords', keywords)\n                    llm_processing_notes = llm_processed_memory.get('processing_notes', 'LLM processed content.')\n                    # Ensure content_to_store is a JSON string before passing to save\n                    if isinstance(content_to_store, dict):\n                        content_to_store_json = json.dumps(content_to_store)\n                    else:\n                        content_to_store_json = str(content_to_store) # Fallback to string if not dict\n                    \n                    self.store_memory(\n                        id=str(uuid.uuid4()),\n                        timestamp=str(rospy.get_time()),\n                        category=category,\n                        content=content_to_store_json,\n                        keywords=keywords_to_store,\n                        source_node=source_node,\n                        salience=salience,\n                        llm_processing_notes=llm_processing_notes,\n                        original_context_json=json.dumps(original_context_snapshot)\n                    )\n                    memory_response_code = 200\n                else:\n                    rospy.logwarn(f\"{self.node_name}: LLM failed to process memory for storage. Storing raw content. (ID: {request_id})\")\n                    self.store_memory(\n                        id=str(uuid.uuid4()),\n                        timestamp=str(rospy.get_time()),\n                        category=category,\n                        content=content_json, # Store original raw JSON string\n                        keywords=keywords,\n                        source_node=source_node,\n                        salience=salience,\n                        llm_processing_notes=\"LLM processing failed, raw content stored.\",\n                        original_context_json=json.dumps(original_context_snapshot)\n                    )\n                    memory_response_code = 200\n            else:\n                rospy.logdebug(f\"{self.node_name}: Insufficient salience for LLM memory processing for store. Storing raw content.\")\n                self.store_memory(\n                    id=str(uuid.uuid4()),\n                    timestamp=str(rospy.get_time()),\n                    category=category,\n                    content=content_json, # Store original raw JSON string\n                    keywords=keywords,\n                    source_node=source_node,\n                    salience=salience,\n                    llm_processing_notes=\"Low salience, raw content stored.\",\n                    original_context_json=json.dumps(original_context_snapshot)\n                )\n                memory_response_code = 200\n            self.cumulative_memory_salience = 0.0 # Reset after store\n\n        elif request_type == 'retrieve':\n            # LLM can assist with semantic search and re-ranking or summarizing retrieved memories\n            if self.cumulative_memory_salience >= self.llm_memory_threshold_salience:\n                rospy.loginfo(f\"{self.node_name}: Triggering LLM for semantic memory retrieval (ID: {request_id}, Query: '{query_text}').\")\n                # First, retrieve a broader set of potentially relevant memories from DB\n                raw_retrieved = self.retrieve_memories_from_db(query_text, category, keywords, num_results * 2) # Get more for LLM re-ranking\n                \n                if raw_retrieved:\n                    llm_retrieved = await self._semantic_retrieve_memories_llm(query_text, raw_retrieved, num_results, original_context_snapshot)\n                    if llm_retrieved:\n                        retrieved_memories = llm_retrieved.get('ranked_memories', [])\n                        llm_processing_notes = llm_retrieved.get('retrieval_notes', 'LLM assisted retrieval.')\n                        memory_response_code = 200\n                    else:\n                        rospy.logwarn(f\"{self.node_name}: LLM semantic retrieval failed. Returning raw database results. (ID: {request_id})\")\n                        retrieved_memories = raw_retrieved[:num_results]\n                        llm_processing_notes = \"LLM retrieval failed, returning simple DB results.\"\n                        memory_response_code = 200\n                else:\n                    rospy.logdebug(f\"{self.node_name}: No raw memories found for query '{query_text}'.\")\n                    memory_response_code = 404 # Not found\n            else:\n                rospy.logdebug(f\"{self.node_name}: Insufficient salience for LLM semantic retrieval. Performing simple keyword retrieval.\")\n                retrieved_memories = self.retrieve_memories_from_db(query_text, category, keywords, num_results)\n                llm_processing_notes = \"Low salience, simple keyword retrieval.\"\n                memory_response_code = 200 if retrieved_memories else 404\n            self.cumulative_memory_salience = 0.0 # Reset after retrieve\n\n        elif request_type == 'update':\n            # LLM can help understand the update and apply it intelligently\n            if self.cumulative_memory_salience >= self.llm_memory_threshold_salience and salience >= 0.6:\n                rospy.loginfo(f\"{self.node_name}: Triggering LLM for memory update processing (ID: {request_id}).\")\n                existing_memories = self.retrieve_memories_from_db(query_text, category, keywords, 1) # Get the most relevant memory\n                if existing_memories:\n                    updated_content_llm = await self._update_memory_content_llm(existing_memories[0], content_parsed, original_context_snapshot)\n                    if updated_content_llm:\n                        updated_memory_id = existing_memories[0]['id']\n                        content_to_update = updated_content_llm.get('updated_content', content_parsed)\n                        llm_processing_notes = updated_content_llm.get('processing_notes', 'LLM updated content.')\n                        if isinstance(content_to_update, dict):\n                            content_to_update_json = json.dumps(content_to_update)\n                        else:\n                            content_to_update_json = str(content_to_update)\n                        self.update_memory(updated_memory_id, content_to_update_json, keywords, salience, llm_processing_notes, json.dumps(original_context_snapshot))\n                        memory_response_code = 200\n                    else:\n                        rospy.logwarn(f\"{self.node_name}: LLM memory update failed. Attempting simple update. (ID: {request_id})\")\n                        if existing_memories and existing_memories[0].get('id'):\n                            self.update_memory(existing_memories[0]['id'], content_json, keywords, salience, \"LLM update failed, simple update applied.\", json.dumps(original_context_snapshot))\n                            memory_response_code = 200\n                        else:\n                            memory_response_code = 404 # Not found\n                else:\n                    memory_response_code = 404 # Not found\n            else:\n                rospy.logdebug(f\"{self.node_name}: Insufficient salience for LLM memory update. Performing simple update.\")\n                existing_memories = self.retrieve_memories_from_db(query_text, category, keywords, 1)\n                if existing_memories and existing_memories[0].get('id'):\n                    self.update_memory(existing_memories[0]['id'], content_json, keywords, salience, \"Low salience, simple update applied.\", json.dumps(original_context_snapshot))\n                    memory_response_code = 200\n                else:\n                    memory_response_code = 404 # Not found\n            self.cumulative_memory_salience = 0.0 # Reset after update\n\n        elif request_type == 'delete':\n            # Simple delete, LLM not strictly needed but could confirm\n            num_deleted = self.delete_memory(query_text, category, keywords)\n            memory_response_code = 200 if num_deleted > 0 else 404\n            rospy.loginfo(f\"{self.node_name}: Deleted {num_deleted} memories for query '{query_text}'.\")\n            self.cumulative_memory_salience = 0.0 # Reset after delete\n\n        elif request_type == 'summarize':\n            # LLM is essential for summarization\n            if self.cumulative_memory_salience >= self.llm_memory_threshold_salience:\n                rospy.loginfo(f\"{self.node_name}: Triggering LLM for memory summarization (ID: {request_id}, Query: '{query_text}').\")\n                memories_to_summarize = self.retrieve_memories_from_db(query_text, category, keywords, 10) # Get a batch of memories\n                if memories_to_summarize:\n                    summary_output = await self._summarize_memories_llm(memories_to_summarize, original_context_snapshot)\n                    if summary_output:\n                        retrieved_memories = [{\"category\": \"summary\", \"content\": summary_output.get('summary_text', \"Could not generate summary.\"), \"keywords\": \"summary\", \"id\": str(uuid.uuid4()), \"timestamp\": str(rospy.get_time())}]\n                        llm_processing_notes = summary_output.get('summarization_notes', 'LLM generated summary.')\n                        memory_response_code = 200\n                    else:\n                        rospy.logwarn(f\"{self.node_name}: LLM summarization failed. (ID: {request_id})\")\n                        retrieved_memories = []\n                        llm_processing_notes = \"LLM summarization failed.\"\n                        memory_response_code = 500 # Internal error\n                else:\n                    rospy.logwarn(f\"{self.node_name}: No memories found to summarize for query '{query_text}'.\")\n                    memory_response_code = 404 # Not found\n            else:\n                rospy.logwarn(f\"{self.node_name}: Insufficient salience for LLM summarization. Summarization skipped.\")\n                memory_response_code = 400 # Bad request, or not enough salience\n            self.cumulative_memory_salience = 0.0 # Reset after summarize\n\n        else:\n            rospy.logwarn(f\"{self.node_name}: Unknown memory request type: {request_type} for ID: {request_id}.\")\n            memory_response_code = 400 # Bad Request\n\n        # Publish the response\n        self.publish_memory_response(\n            request_id=request_id,\n            response_code=memory_response_code,\n            memories_json=json.dumps(retrieved_memories) # Send back as JSON string\n        )\n\n    async def _process_memory_content_llm(self, content_dict, category, keywords_hint, context_snapshot):\n        \"\"\"\n        Uses LLM to process new memory content for better categorization, keyword extraction, or summarization before storage.\n        \"\"\"\n        prompt_text = f\"\"\"\n        You are the Memory Node's LLM assistant. Your task is to process incoming raw memory content to enhance its quality for storage and future retrieval. Analyze the `raw_content` provided, considering the `category_hint` and `keywords_hint`.\n\n        Raw Memory Content:\n        --- Raw Content ---\n        {json.dumps(content_dict, indent=2)}\n\n        Category Hint: '{category}'\n        Keywords Hint: '{keywords_hint}'\n\n        Robot's Current Cognitive Context (for better understanding of memory's significance):\n        --- Cognitive Context ---\n        {json.dumps(context_snapshot, indent=2)}\n\n        Based on this, propose:\n        1.  `processed_content`: object (A refined, possibly summarized or structured version of the content, if beneficial. If the content is already good, return it as is. For text, ensure a 'text' key. For complex data, structure as a meaningful JSON object.)\n        2.  `extracted_keywords`: string (A comma-separated list of highly relevant keywords for this memory, improving searchability. Incorporate `keywords_hint` but expand or refine.)\n        3.  `processing_notes`: string (Brief notes on how you processed or understood this memory.)\n\n        Your response must be in JSON format, containing:\n        1.  'timestamp': string (current ROS time)\n        2.  'processed_content': object\n        3.  'extracted_keywords': string\n        4.  'processing_notes': string\n        \"\"\"\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"timestamp\": {\"type\": \"string\"},\n                \"processed_content\": {\"type\": \"object\"}, # Flexible JSON structure\n                \"extracted_keywords\": {\"type\": \"string\"},\n                \"processing_notes\": {\"type\": \"string\"}\n            },\n            \"required\": [\"timestamp\", \"processed_content\", \"extracted_keywords\", \"processing_notes\"]\n        }\n\n        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.2, max_tokens=600)\n\n        if not llm_output_str.startswith(\"Error:\"):\n            try:\n                llm_data = json.loads(llm_output_str)\n                return llm_data\n            except json.JSONDecodeError as e:\n                self._report_error(\"LLM_PARSE_ERROR\", f\"Failed to parse LLM response for memory processing: {e}. Raw: {llm_output_str}\", 0.8)\n                return None\n        else:\n            self._report_error(\"LLM_MEMORY_PROCESS_FAILED\", f\"LLM call failed for memory processing: {llm_output_str}\", 0.9)\n            return None\n\n    async def _semantic_retrieve_memories_llm(self, query_text, raw_memories, num_results, context_snapshot):\n        \"\"\"\n        Uses LLM to perform semantic retrieval and re-ranking of memories.\n        `raw_memories` are initial candidates retrieved by simple keyword matching.\n        \"\"\"\n        memories_str_for_llm = \"\\n\".join([f\"- Memory ID: {m.get('id')}\\n  Category: {m.get('category')}\\n  Keywords: {m.get('keywords')}\\n  Content Summary: {m.get('content', '')[:150]}...\" for m in raw_memories])\n\n        prompt_text = f\"\"\"\n        You are the Memory Node's LLM assistant. Your task is to semantically retrieve and rank relevant memories based on a `user_query` from a list of `candidate_memories`. Also consider the robot's `current_cognitive_context` for a more nuanced understanding of the query's intent.\n\n        User Query: '{query_text}'\n\n        Candidate Memories (from initial database search):\n        --- Candidate Memories ---\n        {memories_str_for_llm}\n\n        Robot's Current Cognitive Context (for query intent and relevance):\n        --- Cognitive Context ---\n        {json.dumps(context_snapshot, indent=2)}\n\n        Based on the `user_query` and `current_cognitive_context`, identify the {num_results} most relevant memories from the `candidate_memories`.\n        Provide your response in JSON format, containing:\n        1.  `timestamp`: string (current ROS time)\n        2.  `ranked_memories`: array of objects (The selected relevant memories, each including their full original content and metadata. Ordered by relevance, highest first.)\n        3.  `retrieval_notes`: string (Brief explanation for the selection and ranking.)\n\n        Ensure each memory in `ranked_memories` includes its 'id', 'timestamp', 'category', 'content', 'keywords', 'source_node', 'salience'.\n        \"\"\"\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"timestamp\": {\"type\": \"string\"},\n                \"ranked_memories\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"id\": {\"type\": \"string\"},\n                            \"timestamp\": {\"type\": \"string\"},\n                            \"category\": {\"type\": \"string\"},\n                            \"content\": {\"type\": \"string\"},\n                            \"keywords\": {\"type\": \"string\"},\n                            \"source_node\": {\"type\": \"string\"},\n                            \"salience\": {\"type\": \"number\"}\n                        },\n                        \"required\": [\"id\", \"timestamp\", \"category\", \"content\", \"keywords\", \"source_node\", \"salience\"]\n                    }\n                },\n                \"retrieval_notes\": {\"type\": \"string\"}\n            },\n            \"required\": [\"timestamp\", \"ranked_memories\", \"retrieval_notes\"]\n        }\n\n        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.1, max_tokens=800)\n\n        if not llm_output_str.startswith(\"Error:\"):\n            try:\n                llm_data = json.loads(llm_output_str)\n                # Ensure salience is float\n                if 'ranked_memories' in llm_data:\n                    for mem in llm_data['ranked_memories']:\n                        if 'salience' in mem: mem['salience'] = float(mem['salience'])\n                return llm_data\n            except json.JSONDecodeError as e:\n                self._report_error(\"LLM_PARSE_ERROR\", f\"Failed to parse LLM response for semantic retrieval: {e}. Raw: {llm_output_str}\", 0.8)\n                return None\n        else:\n            self._report_error(\"LLM_SEMANTIC_RETRIEVAL_FAILED\", f\"LLM call failed for semantic retrieval: {llm_output_str}\", 0.9)\n            return None\n\n    async def _update_memory_content_llm(self, existing_memory, new_content_dict, context_snapshot):\n        \"\"\"\n        Uses LLM to intelligently update a memory's content, merging new information.\n        \"\"\"\n        prompt_text = f\"\"\"\n        You are the Memory Node's LLM assistant. Your task is to intelligently update an existing memory's content with new information. You need to integrate the `new_content` into the `existing_memory`, ensuring logical consistency and preserving important details.\n\n        Existing Memory to Update:\n        --- Existing Memory ---\n        {json.dumps(existing_memory, indent=2)}\n\n        New Content to Integrate:\n        --- New Content ---\n        {json.dumps(new_content_dict, indent=2)}\n\n        Robot's Current Cognitive Context (for understanding the update's purpose):\n        --- Cognitive Context ---\n        {json.dumps(context_snapshot, indent=2)}\n\n        Based on this, generate:\n        1.  `updated_content`: object (The merged and updated content. Ensure it's a valid JSON object. If the content is text-based, make sure it has a 'text' key.)\n        2.  `processing_notes`: string (Brief notes on how you performed the update or any conflicts encountered.)\n\n        Your response must be in JSON format, containing:\n        1.  'timestamp': string (current ROS time)\n        2.  'updated_content': object\n        3.  'processing_notes': string\n        \"\"\"\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"timestamp\": {\"type\": \"string\"},\n                \"updated_content\": {\"type\": \"object\"},\n                \"processing_notes\": {\"type\": \"string\"}\n            },\n            \"required\": [\"timestamp\", \"updated_content\", \"processing_notes\"]\n        }\n\n        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.2, max_tokens=700)\n\n        if not llm_output_str.startswith(\"Error:\"):\n            try:\n                llm_data = json.loads(llm_output_str)\n                return llm_data\n            except json.JSONDecodeError as e:\n                self._report_error(\"LLM_PARSE_ERROR\", f\"Failed to parse LLM response for memory update: {e}. Raw: {llm_output_str}\", 0.8)\n                return None\n        else:\n            self._report_error(\"LLM_MEMORY_UPDATE_FAILED\", f\"LLM call failed for memory update: {llm_output_str}\", 0.9)\n            return None\n\n    async def _summarize_memories_llm(self, memories_list, context_snapshot):\n        \"\"\"\n        Uses LLM to summarize a list of memories into a concise overview.\n        \"\"\"\n        memories_for_llm = \"\\n\".join([f\"- ID: {m.get('id')}, Category: {m.get('category')}, Content: {m.get('content', '')[:200]}...\" for m in memories_list])\n\n        prompt_text = f\"\"\"\n        You are the Memory Node's LLM assistant. Your task is to summarize a collection of `memories_list` into a concise and coherent overview. Consider the robot's `current_cognitive_context` to identify the most salient aspects for the summary.\n\n        Memories to Summarize:\n        --- Memories ---\n        {memories_for_llm}\n\n        Robot's Current Cognitive Context (for identifying key themes for summary):\n        --- Cognitive Context ---\n        {json.dumps(context_snapshot, indent=2)}\n\n        Based on these memories and context, generate:\n        1.  `summary_text`: string (A concise and coherent summary of the provided memories. Focus on key events, facts, or patterns.)\n        2.  `summarization_notes`: string (Brief notes on the key themes or insights gained during summarization.)\n\n        Your response must be in JSON format, containing:\n        1.  'timestamp': string (current ROS time)\n        2.  'summary_text': string\n        3.  'summarization_notes': string\n        \"\"\"\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"timestamp\": {\"type\": \"string\"},\n                \"summary_text\": {\"type\": \"string\"},\n                \"summarization_notes\": {\"type\": \"string\"}\n            },\n            \"required\": [\"timestamp\", \"summary_text\", \"summarization_notes\"]\n        }\n\n        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.3, max_tokens=400) # Slightly higher temp for summarization nuance\n\n        if not llm_output_str.startswith(\"Error:\"):\n            try:\n                llm_data = json.loads(llm_output_str)\n                return llm_data\n            except json.JSONDecodeError as e:\n                self._report_error(\"LLM_PARSE_ERROR\", f\"Failed to parse LLM response for summarization: {e}. Raw: {llm_output_str}\", 0.8)\n                return None\n        else:\n            self._report_error(\"LLM_SUMMARIZATION_FAILED\", f\"LLM call failed for summarization: {llm_output_str}\", 0.9)\n            return None\n\n\n    def _compile_llm_context_for_memory_op(self, request_data):\n        \"\"\"\n        Gathers and formats all relevant cognitive state data for the LLM's\n        memory processing operations.\n        \"\"\"\n        context = {\n            \"current_time\": rospy.get_time(),\n            \"memory_request_details\": request_data,\n            \"recent_cognitive_inputs\": {\n                \"cognitive_directives_for_self\": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name],\n                \"internal_narratives\": list(self.recent_internal_narratives),\n                \"world_model_states\": list(self.recent_world_model_states),\n                \"social_cognition_states\": list(self.recent_social_cognition_states)\n            }\n        }\n        \n        # Deep parse any nested JSON strings in context for better LLM understanding\n        for category_key in context[\"recent_cognitive_inputs\"]:\n            for i, item in enumerate(context[\"recent_cognitive_inputs\"][category_key]):\n                if isinstance(item, dict):\n                    for field, value in item.items():\n                        if isinstance(value, str) and field.endswith('_json'):\n                            try:\n                                item[field] = json.loads(value)\n                            except json.JSONDecodeError:\n                                pass # Keep as string if not valid JSON\n        return context\n\n    # --- Database Operations (SQLite) ---\n    def store_memory(self, id, timestamp, category, content, keywords, source_node, salience, llm_processing_notes, original_context_json):\n        \"\"\"Stores a memory in the SQLite database.\"\"\"\n        try:\n            self.cursor.execute('''\n                INSERT INTO memories (id, timestamp, category, content, keywords, source_node, salience, llm_processing_notes, original_context_json)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (id, timestamp, category, content, keywords, source_node, salience, llm_processing_notes, original_context_json))\n            self.conn.commit()\n            rospy.loginfo(f\"{self.node_name}: Stored memory (ID: {id}, Category: {category}).\")\n        except sqlite3.Error as e:\n            self._report_error(\"DB_STORE_ERROR\", f\"Failed to store memory: {e}\", 0.9)\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_STORE_ERROR\", f\"Unexpected error in store_memory: {e}\", 0.9)\n\n    def retrieve_memories_from_db(self, query_text, category_filter=None, keywords_filter=None, num_results=5):\n        \"\"\"Retrieves memories from the SQLite database based on query and filters.\"\"\"\n        # Simple keyword matching for initial retrieval. LLM handles semantic.\n        query = \"SELECT id, timestamp, category, content, keywords, source_node, salience, llm_processing_notes FROM memories WHERE 1=1\"\n        params = []\n\n        if query_text:\n            query += \" AND (content LIKE ? OR keywords LIKE ?)\"\n            params.extend([f\"%{query_text}%\", f\"%{query_text}%\"])\n        if category_filter and category_filter != 'general':\n            query += \" AND category = ?\"\n            params.append(category_filter)\n        if keywords_filter: # This assumes comma-separated keywords in DB\n            keywords_list = [k.strip() for k in keywords_filter.split(',') if k.strip()]\n            keyword_conditions = [f\"keywords LIKE ?\" for _ in keywords_list]\n            if keyword_conditions:\n                query += \" AND (\" + \" OR \".join(keyword_conditions) + \")\"\n                params.extend([f\"%{k}%\" for k in keywords_list]) # Use LIKE for partial matches\n\n        query += \" ORDER BY salience DESC, timestamp DESC LIMIT ?\"\n        params.append(num_results)\n\n        try:\n            self.cursor.execute(query, params)\n            rows = self.cursor.fetchall()\n            retrieved = []\n            for row in rows:\n                mem = {\n                    'id': row[0],\n                    'timestamp': row[1],\n                    'category': row[2],\n                    'content': row[3], # Raw content string\n                    'keywords': row[4],\n                    'source_node': row[5],\n                    'salience': row[6],\n                    'llm_processing_notes': row[7]\n                }\n                # Attempt to parse content back to dict if it was stored as JSON string\n                try:\n                    mem['content_parsed'] = json.loads(mem['content'])\n                except (json.JSONDecodeError, TypeError):\n                    mem['content_parsed'] = mem['content'] # Keep as string if not JSON\n                retrieved.append(mem)\n            rospy.loginfo(f\"{self.node_name}: Retrieved {len(retrieved)} memories for query '{query_text}'.\")\n            return retrieved\n        except sqlite3.Error as e:\n            self._report_error(\"DB_RETRIEVE_ERROR\", f\"Failed to retrieve memories: {e}\", 0.9, {'query_text': query_text, 'category': category_filter})\n            return []\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_RETRIEVE_ERROR\", f\"Unexpected error in retrieve_memories_from_db: {e}\", 0.9)\n            return []\n\n    def update_memory(self, memory_id, new_content_json, new_keywords, new_salience, llm_processing_notes, original_context_json):\n        \"\"\"Updates an existing memory in the SQLite database.\"\"\"\n        try:\n            self.cursor.execute('''\n                UPDATE memories\n                SET content = ?, keywords = ?, salience = ?, llm_processing_notes = ?, original_context_json = ?\n                WHERE id = ?\n            ''', (new_content_json, new_keywords, new_salience, llm_processing_notes, original_context_json, memory_id))\n            self.conn.commit()\n            if self.cursor.rowcount > 0:\n                rospy.loginfo(f\"{self.node_name}: Updated memory (ID: {memory_id}).\")\n                return True\n            else:\n                rospy.logwarn(f\"{self.node_name}: No memory found with ID: {memory_id} for update.\")\n                return False\n        except sqlite3.Error as e:\n            self._report_error(\"DB_UPDATE_ERROR\", f\"Failed to update memory {memory_id}: {e}\", 0.9)\n            return False\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_UPDATE_ERROR\", f\"Unexpected error in update_memory: {e}\", 0.9)\n            return False\n\n    def delete_memory(self, query_text, category_filter=None, keywords_filter=None):\n        \"\"\"Deletes memories from the SQLite database.\"\"\"\n        query = \"DELETE FROM memories WHERE 1=1\"\n        params = []\n\n        # For simplicity, deletion uses keyword/text matching similar to retrieval\n        if query_text:\n            query += \" AND (content LIKE ? OR keywords LIKE ?)\"\n            params.extend([f\"%{query_text}%\", f\"%{query_text}%\"])\n        if category_filter and category_filter != 'general':\n            query += \" AND category = ?\"\n            params.append(category_filter)\n        if keywords_filter:\n            keywords_list = [k.strip() for k in keywords_filter.split(',') if k.strip()]\n            keyword_conditions = [f\"keywords LIKE ?\" for _ in keywords_list]\n            if keyword_conditions:\n                query += \" AND (\" + \" OR \".join(keyword_conditions) + \")\"\n                params.extend([f\"%{k}%\" for k in keywords_list])\n\n        try:\n            self.cursor.execute(query, params)\n            self.conn.commit()\n            rospy.loginfo(f\"{self.node_name}: Deleted {self.cursor.rowcount} memories.\")\n            return self.cursor.rowcount\n        except sqlite3.Error as e:\n            self._report_error(\"DB_DELETE_ERROR\", f\"Failed to delete memories: {e}\", 0.9, {'query_text': query_text, 'category': category_filter})\n            return 0\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_DELETE_ERROR\", f\"Unexpected error in delete_memory: {e}\", 0.9)\n            return 0\n\n    # --- Publishing Functions ---\n    def publish_memory_response(self, request_id, response_code, memories_json):\n        \"\"\"Publishes the response to a memory request.\"\"\"\n        timestamp = str(rospy.get_time())\n        try:\n            if isinstance(MemoryResponse, type(String)): # Fallback to String message\n                response_data = {\n                    'timestamp': timestamp,\n                    'request_id': request_id,\n                    'response_code': response_code,\n                    'memories_json': memories_json # Already JSON string\n                }\n                self.pub_memory_response.publish(json.dumps(response_data))\n            else:\n                response_msg = MemoryResponse()\n                response_msg.timestamp = timestamp\n                response_msg.request_id = request_id\n                response_msg.response_code = response_code\n                response_msg.memories_json = memories_json\n                self.pub_memory_response.publish(response_msg)\n\n            rospy.logdebug(f\"{self.node_name}: Published Memory Response for request ID: {request_id}. Code: {response_code}.\")\n\n        except Exception as e:\n            self._report_error(\"PUBLISH_MEMORY_RESPONSE_ERROR\", f\"Failed to publish memory response for ID '{request_id}': {e}\", 0.7)\n\n\n    def run(self):\n        \"\"\"Starts the ROS node and keeps it spinning.\"\"\"\n        rospy.spin()\n\n    def __del__(self):\n        \"\"\"Ensures the database connection is closed on node shutdown and async loop is stopped.\"\"\"\n        rospy.loginfo(f\"{self.node_name} shutting down. Closing database connection and asyncio loop.\")\n        if hasattr(self, 'conn') and self.conn:\n            self.conn.close()\n        self._shutdown_async_loop()\n\nif __name__ == '__main__':\n    try:\n        node = MemoryNode()\n        node.run()\n    except rospy.ROSInterruptException:\n        rospy.loginfo(f\"{rospy.get_name()} interrupted by ROS shutdown.\")\n        if 'node' in locals() and isinstance(node, MemoryNode):\n            node._shutdown_async_loop()\n            if hasattr(node, 'conn'): node.conn.close()\n    except Exception as e:\n        rospy.logerr(f\"{rospy.get_name()} encountered an unexpected error: {e}\")\n        if 'node' in locals() and isinstance(node, MemoryNode):\n            node._shutdown_async_loop()\n            if hasattr(node, 'conn'): node.conn.close()","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}