{"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\nimport rospy\nimport sqlite3\nimport os\nimport json\nimport time\nimport random\nimport uuid # For unique qualia IDs\n\n# --- Asyncio Imports for LLM calls ---\nimport asyncio\nimport aiohttp\nimport threading\nfrom collections import deque\n\nfrom std_msgs.msg import String\n\n# Updated imports for custom messages:\ntry:\n    from sentience.msg import (\n        SensoryQualia,          # Output: Processed sensory data with qualia attributes\n        RawSensorData,          # Input: Raw sensor data (e.g., camera, microphone, lidar)\n        CognitiveDirective,     # Input: Directives for sensory processing focus\n        AttentionState,         # Input: Current attention focus (influences salience assessment)\n        WorldModelState         # Input: Current world state (context for sensory interpretation)\n    )\nexcept ImportError:\n    rospy.logwarn(\"Custom ROS messages for 'sentience' package not found. Using String for all incoming/outgoing data for fallback in Sensory Qualia Node.\")\n    SensoryQualia = String\n    RawSensorData = String\n    CognitiveDirective = String\n    AttentionState = String\n    WorldModelState = String\n    String = String # Ensure String is defined even if other custom messages aren't\n\n# --- Import shared utility functions ---\n# Assuming 'sentience/scripts/utils.py' exists and contains parse_ros_message_data and load_config\ntry:\n    from sentience.scripts.utils import parse_ros_message_data, load_config\nexcept ImportError:\n    rospy.logwarn(\"Could not import sentience.scripts.utils. Using fallback for parse_ros_message_data and load_config.\")\n    # Fallback implementations if the utility file isn't available\n    def parse_ros_message_data(msg, fields_map, node_name=\"unknown_node\"):\n        \"\"\"\n        Fallback parser for ROS messages, assuming String message and JSON content.\n        If msg is not String, it attempts to access attributes directly.\n        \"\"\"\n        data = {}\n        if isinstance(msg, String):\n            try:\n                parsed_json = json.loads(msg.data)\n                for key_in_msg, (default_val, target_key) in fields_map.items():\n                    data[target_key] = parsed_json.get(key_in_msg, default_val)\n            except json.JSONDecodeError:\n                rospy.logerr(f\"{node_name}: Could not parse String message data as JSON: {msg.data}\")\n                for key_in_msg, (default_val, target_key) in fields_map.items():\n                    data[target_key] = default_val # Use defaults on JSON error\n            else:\n                # Attempt to get attributes directly from the message object\n                for key_in_msg, (default_val, target_key) in fields_map.items():\n                    data[target_key] = getattr(msg, key_in_msg, default_val)\n        return data\n\n    def load_config(node_name, config_path):\n        \"\"\"\n        Fallback config loader: returns hardcoded defaults.\n        In a real scenario, this should load from a YAML file.\n        \"\"\"\n        rospy.logwarn(f\"{node_name}: Using hardcoded default configuration as '{config_path}' could not be loaded.\")\n        return {\n            'db_root_path': '/tmp/sentience_db',\n            'default_log_level': 'INFO',\n            'sensory_qualia_node': {\n                'processing_interval': 0.1, # How often to process incoming raw sensor data\n                'llm_interpretation_threshold_salience': 0.7, # Cumulative salience to trigger LLM\n                'recent_context_window_s': 5.0 # Window for deques for LLM context (short for real-time sensory)\n            },\n            'llm_params': { # Global LLM parameters for fallback\n                'model_name': \"phi-2\",\n                'base_url': \"http://localhost:8000/v1/chat/completions\",\n                'timeout_seconds': 15.0\n            }\n        }.get(node_name, {}) # Return node-specific or empty dict\n\n\nclass SensoryQualiaNode:\n    def __init__(self):\n        rospy.init_node('sensory_qualia_node', anonymous=False)\n        self.node_name = rospy.get_name()\n\n        # --- Load parameters from centralized config ---\n        config_file_path = rospy.get_param('~config_file_path', None)\n        if config_file_path is None:\n            rospy.logfatal(f\"{self.node_name}: 'config_file_path' parameter is not set. Cannot load configuration. Shutting down.\")\n            rospy.signal_shutdown(\"Missing config_file_path parameter.\")\n            return\n\n        full_config = load_config(\"global\", config_file_path) # Load global params\n        self.params = load_config(self.node_name.strip('/'), config_file_path) # Load node-specific params\n\n        if not self.params or not full_config:\n            rospy.logfatal(f\"{self.node_name}: Failed to load configuration from '{config_file_path}'. Shutting down.\")\n            rospy.signal_shutdown(\"Configuration loading failed.\")\n            return\n\n        # Assign parameters\n        self.db_path = os.path.join(full_config.get('db_root_path', '/tmp/sentience_db'), \"sensory_log.db\")\n        self.processing_interval = self.params.get('processing_interval', 0.1) # How often to process raw sensor data\n        self.llm_interpretation_threshold_salience = self.params.get('llm_interpretation_threshold_salience', 0.7) # Salience to trigger LLM\n        self.recent_context_window_s = self.params.get('recent_context_window_s', 5.0) # Context window for LLM\n\n        # LLM Parameters (from global config)\n        self.llm_model_name = full_config.get('llm_params', {}).get('model_name', \"phi-2\")\n        self.llm_base_url = full_config.get('llm_params', {}).get('base_url', \"http://localhost:8000/v1/chat/completions\")\n        self.llm_timeout = full_config.get('llm_params', {}).get('timeout_seconds', 15.0) # Timeout for LLM calls\n\n        # Set ROS log level from config\n        rospy.set_param('/rosout/log_level', full_config.get('default_log_level', 'INFO').upper())\n\n\n        # --- Asyncio Setup ---\n        self._async_loop = asyncio.new_event_loop()\n        self._async_thread = threading.Thread(target=self._run_async_loop, daemon=True)\n        self._async_thread.start()\n        self._async_session = None\n        self.active_llm_task = None # To track the currently running LLM task\n\n        # --- Initialize SQLite database ---\n        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)\n        self.cursor = self.conn.cursor()\n\n        # Create the 'sensory_log' table if it doesn't exist.\n        # NEW: Added 'llm_interpretation_notes', 'raw_data_hash'\n        self.cursor.execute('''\n            CREATE TABLE IF NOT EXISTS sensory_log (\n                id TEXT PRIMARY KEY,            -- Unique qualia ID (UUID)\n                timestamp TEXT,\n                qualia_type TEXT,               -- e.g., 'visual_perception', 'auditory_stimulus', 'tactile_sensation'\n                modality TEXT,                  -- e.g., 'camera', 'microphone', 'lidar', 'touch_sensor'\n                description_summary TEXT,       -- Concise summary of the sensory experience\n                salience_score REAL,            -- How attention-grabbing this qualia is (0.0 to 1.0)\n                llm_interpretation_notes TEXT,  -- NEW: LLM's detailed interpretation of the qualia\n                raw_data_hash TEXT,             -- HASH of the raw input data for traceability (e.g., md5 of image)\n                context_snapshot_json TEXT      -- NEW: JSON of relevant cognitive context at time of interpretation\n            )\n        ''')\n        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_sensory_timestamp ON sensory_log (timestamp)')\n        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_sensory_modality ON sensory_log (modality)')\n        self.conn.commit() # Commit schema changes\n\n        # --- Internal State ---\n        self.raw_sensor_data_queue = deque() # Stores incoming raw sensor data for processing\n\n        # Deques to maintain a short history of inputs relevant to sensory interpretation\n        self.recent_cognitive_directives = deque(maxlen=3) # Directives for sensory processing focus\n        self.recent_attention_states = deque(maxlen=3) # Current attention focus (influences salience)\n        self.recent_world_model_states = deque(maxlen=3) # Context for interpreting new sensory data\n\n        self.cumulative_sensory_salience = 0.0 # Aggregated salience to trigger LLM interpretation\n\n        # --- Publishers ---\n        self.pub_sensory_qualia = rospy.Publisher('/sensory_qualia', SensoryQualia, queue_size=10)\n        self.pub_error_report = rospy.Publisher('/error_monitor/report', String, queue_size=10)\n        self.pub_cognitive_directive = rospy.Publisher('/cognitive_directives', CognitiveDirective, queue_size=10) # To request attention or memory\n\n\n        # --- Subscribers ---\n        rospy.Subscriber('/raw_sensor_data', RawSensorData, self.raw_sensor_data_callback)\n        rospy.Subscriber('/cognitive_directives', CognitiveDirective, self.cognitive_directive_callback)\n        rospy.Subscriber('/attention_state', AttentionState, self.attention_state_callback)\n        rospy.Subscriber('/world_model_state', String, self.world_model_state_callback) # Stringified JSON\n\n\n        # --- Timer for periodic sensory processing ---\n        rospy.Timer(rospy.Duration(self.processing_interval), self._run_sensory_processing_wrapper)\n\n        rospy.loginfo(f\"{self.node_name}: Robot's sensory qualia system online, ready to interpret perceptions.\")\n\n    # --- Asyncio Thread Management ---\n    def _run_async_loop(self):\n        asyncio.set_event_loop(self._async_loop)\n        self._async_loop.run_until_complete(self._create_async_session())\n        self._async_loop.run_forever()\n\n    async def _create_async_session(self):\n        rospy.loginfo(f\"{self.node_name}: Creating aiohttp ClientSession...\")\n        self._async_session = aiohttp.ClientSession()\n        rospy.loginfo(f\"{self.node_name}: aiohttp ClientSession created.\")\n\n    async def _close_async_session(self):\n        if self._async_session:\n            rospy.loginfo(f\"{self.node_name}: Closing aiohttp ClientSession...\")\n            await self._async_session.close()\n            self._async_session = None\n            rospy.loginfo(f\"{self.node_name}: aiohttp ClientSession closed.\")\n\n    def _shutdown_async_loop(self):\n        if self._async_loop and self._async_thread.is_alive():\n            rospy.loginfo(f\"{self.node_name}: Shutting down asyncio loop...\")\n            future = asyncio.run_coroutine_threadsafe(self._close_async_session(), self._async_loop)\n            try:\n                future.result(timeout=5.0)\n            except asyncio.TimeoutError:\n                rospy.logwarn(f\"{self.node_name}: Timeout waiting for async session to close.\")\n            self._async_loop.call_soon_threadsafe(self._async_loop.stop)\n            self._async_thread.join(timeout=5.0)\n            if self._async_thread.is_alive():\n                rospy.logwarn(f\"{self.node_name}: Asyncio thread did not shut down gracefully.\")\n            rospy.loginfo(f\"{self.node_name}: Asyncio loop shut down.\")\n\n    def _run_sensory_processing_wrapper(self, event):\n        \"\"\"Wrapper to run the async sensory processing from a ROS timer.\"\"\"\n        if self.active_llm_task and not self.active_llm_task.done():\n            rospy.logdebug(f\"{self.node_name}: LLM sensory processing task already active. Skipping new cycle.\")\n            return\n\n        if self.raw_sensor_data_queue:\n            raw_data = self.raw_sensor_data_queue.popleft()\n            self.active_llm_task = asyncio.run_coroutine_threadsafe(\n                self.process_raw_sensor_data_async(raw_data, event), self._async_loop\n            )\n        else:\n            rospy.logdebug(f\"{self.node_name}: No raw sensor data in queue.\")\n\n    # --- Error Reporting Utility ---\n    def _report_error(self, error_type, description, severity=0.5, context=None):\n        timestamp = str(rospy.get_time())\n        error_msg_data = {\n            'timestamp': timestamp, 'source_node': self.node_name, 'error_type': error_type,\n            'description': description, 'severity': severity, 'context': context if context else {}\n        }\n        try:\n            self.pub_error_report.publish(json.dumps(error_msg_data))\n            rospy.logerr(f\"{self.node_name}: REPORTED ERROR: {error_type} - {description}\")\n        except Exception as e:\n            rospy.logerr(f\"{self.node_name}: Failed to publish error report: {e}\")\n\n    # --- LLM Call Function (ADAPTED FOR LOCAL PHI-2 SERVER) ---\n    async def _call_llm_api(self, prompt_text, response_schema=None, temperature=0.3, max_tokens=200):\n        \"\"\"\n        Asynchronously calls the local LLM inference server (e.g., llama.cpp compatible API).\n        Can optionally request a structured JSON response. Low temperature for factual interpretation.\n        \"\"\"\n        if not self._async_session:\n            await self._create_async_session() # Attempt to create if not exists\n            if not self._async_session:\n                self._report_error(\"LLM_SESSION_ERROR\", \"aiohttp session not available for LLM call.\", 0.8)\n                return \"Error: LLM session not ready.\"\n\n        payload = {\n            \"model\": self.llm_model_name,\n            \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n            \"temperature\": temperature, # Low temperature for factual interpretation of sensory data\n            \"max_tokens\": max_tokens,\n            \"stream\": False\n        }\n        headers = {'Content-Type': 'application/json'}\n\n        if response_schema:\n            prompt_text += \"\\n\\nProvide the response in JSON format according to this schema:\\n\" + json.dumps(response_schema, indent=2)\n            payload[\"messages\"] = [{\"role\": \"user\", \"content\": prompt_text}]\n\n        api_url = self.llm_base_url\n\n        try:\n            async with self._async_session.post(api_url, json=payload, timeout=self.llm_timeout, headers=headers) as response:\n                response.raise_for_status() # Raise an exception for bad status codes\n                result = await response.json()\n\n                if result.get('choices') and result['choices'][0].get('message') and \\\n                   result['choices'][0]['message'].get('content'):\n                    return result['choices'][0]['message']['content']\n                \n                self._report_error(\"LLM_RESPONSE_EMPTY\", \"LLM response had no content from local server.\", 0.5, {'prompt_snippet': prompt_text[:100], 'raw_result': str(result)})\n                return \"Error: LLM response empty.\"\n        except aiohttp.ClientError as e:\n            self._report_error(\"LLM_API_ERROR\", f\"LLM API request failed (aiohttp ClientError to local server): {e}\", 0.9, {'url': api_url})\n            return f\"Error: LLM API request failed: {e}\"\n        except asyncio.TimeoutError:\n            self._report_error(\"LLM_TIMEOUT\", f\"LLM API request timed out after {self.llm_timeout} seconds (local server).\", 0.8, {'prompt_snippet': prompt_text[:100]})\n            return \"Error: LLM API request timed out.\"\n        except json.JSONDecodeError:\n            self._report_error(\"LLM_JSON_PARSE_ERROR\", \"Failed to parse local LLM response JSON.\", 0.7, {'raw_response': str(result) if 'result' in locals() else 'N/A'})\n            return \"Error: Failed to parse LLM response.\"\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_LLM_ERROR\", f\"An unexpected error occurred during local LLM call: {e}\", 0.9, {'prompt_snippet': prompt_text[:100]})\n            return f\"Error: An unexpected error occurred: {e}\"\n\n    # --- Utility to accumulate input salience ---\n    def _update_cumulative_salience(self, score):\n        \"\"\"Accumulates salience from new inputs for triggering LLM interpretation.\"\"\"\n        self.cumulative_sensory_salience += score\n        self.cumulative_sensory_salience = min(1.0, self.cumulative_sensory_salience) # Clamp at 1.0\n\n    # --- Pruning old history ---\n    def _prune_history(self):\n        \"\"\"Removes old entries from history deques based on recent_context_window_s.\"\"\"\n        current_time = rospy.get_time()\n        for history_deque in [\n            self.recent_cognitive_directives, self.recent_attention_states,\n            self.recent_world_model_states\n        ]:\n            while history_deque and (current_time - float(history_deque[0].get('timestamp', 0.0))) > self.recent_context_window_s:\n                history_deque.popleft()\n\n    # --- Callbacks for incoming data (populate history and accumulate salience) ---\n    def raw_sensor_data_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'sensor_id': ('', 'sensor_id'),\n            'modality': ('unknown', 'modality'), 'raw_data_json': ('{}', 'raw_data_json'),\n            'data_hash': ('', 'data_hash'), 'urgency': (0.0, 'urgency') # Urgency for this specific raw data\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        \n        # Parse raw_data_json if it's a string\n        if isinstance(data.get('raw_data_json'), str):\n            try:\n                data['raw_data_parsed'] = json.loads(data['raw_data_json'])\n            except json.JSONDecodeError:\n                data['raw_data_parsed'] = {} # Fallback if not valid JSON\n        else:\n            data['raw_data_parsed'] = data.get('raw_data_json', {}) # Ensure it's a dict\n\n        self.raw_sensor_data_queue.append(data)\n        # Salience of the raw sensor data influences LLM trigger\n        self._update_cumulative_salience(data.get('urgency', 0.0) * 0.8) # High urgency for direct sensor data\n        rospy.logdebug(f\"{self.node_name}: Queued raw sensor data (Modality: {data['modality']}, Sensor: {data['sensor_id']}). Queue size: {len(self.raw_sensor_data_queue)}.\")\n\n    def cognitive_directive_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'directive_type': ('', 'directive_type'),\n            'target_node': ('', 'target_node'), 'command_payload': ('{}', 'command_payload'),\n            'urgency': (0.0, 'urgency'), 'reason': ('', 'reason')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        \n        if data.get('target_node') == self.node_name and data.get('directive_type') == 'FocusSensoryProcessing':\n            try:\n                payload = json.loads(data.get('command_payload', '{}'))\n                # This directive doesn't go into a queue, it directly influences the next processing cycle\n                self._update_cumulative_salience(data.get('urgency', 0.0) * 0.9) # High urgency for focus directives\n                rospy.loginfo(f\"{self.node_name}: Received directive to focus sensory processing on reason: '{data.get('reason', 'N/A')}'.\")\n            except json.JSONDecodeError as e:\n                self._report_error(\"JSON_DECODE_ERROR\", f\"Failed to decode command_payload in CognitiveDirective: {e}\", 0.5, {'payload': data.get('command_payload')})\n            except Exception as e:\n                self._report_error(\"DIRECTIVE_PROCESSING_ERROR\", f\"Error processing CognitiveDirective for sensory: {e}\", 0.7, {'directive': data})\n        \n        self.recent_cognitive_directives.append(data) # Store all directives for context\n        rospy.logdebug(f\"{self.node_name}: Cognitive Directive received for context/action.\")\n\n    def attention_state_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'focus_type': ('idle', 'focus_type'),\n            'focus_target': ('environment', 'focus_target'), 'priority_score': (0.0, 'priority_score')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        self.recent_attention_states.append(data)\n        # What the robot is attending to influences what sensory input is considered salient\n        if data.get('priority_score', 0.0) > 0.5:\n            self._update_cumulative_salience(data.get('priority_score', 0.0) * 0.3)\n        rospy.logdebug(f\"{self.node_name}: Received Attention State. Focus: {data.get('focus_target', 'N/A')}.\")\n\n    def world_model_state_callback(self, msg):\n        fields_map = {\n            'timestamp': (str(rospy.get_time()), 'timestamp'), 'num_entities': (0, 'num_entities'),\n            'changed_entities_json': ('[]', 'changed_entities_json'),\n            'significant_change_flag': (False, 'significant_change_flag'),\n            'consistency_score': (1.0, 'consistency_score')\n        }\n        data = parse_ros_message_data(msg, fields_map, node_name=self.node_name)\n        if isinstance(data.get('changed_entities_json'), str):\n            try: data['changed_entities'] = json.loads(data['changed_entities_json'])\n            except json.JSONDecodeError: data['changed_entities'] = []\n        self.recent_world_model_states.append(data)\n        # World model provides context for interpreting sensory data (e.g., expecting a human in a certain area)\n        if data.get('significant_change_flag', False):\n            self._update_cumulative_salience(0.2)\n        rospy.logdebug(f\"{self.node_name}: Received World Model State. Significant Change: {data.get('significant_change_flag', False)}.\")\n\n    # --- Core Sensory Processing Logic (Async with LLM) ---\n    async def process_raw_sensor_data_async(self, raw_data, event):\n        \"\"\"\n        Asynchronously processes raw sensor data to extract 'qualia' (meaningful, salient perceptions),\n        using LLM for higher-level interpretation.\n        \"\"\"\n        self._prune_history() # Keep context history fresh\n\n        qualia_id = str(uuid.uuid4())\n        timestamp = raw_data.get('timestamp', str(rospy.get_time()))\n        modality = raw_data.get('modality', 'unknown')\n        raw_data_hash = raw_data.get('data_hash', '')\n        \n        qualia_type = 'unspecified_perception'\n        description_summary = \"Raw data processed.\"\n        salience_score = 0.0 # Default low, will be updated\n        llm_interpretation_notes = \"No LLM interpretation.\"\n\n        if self.cumulative_sensory_salience >= self.llm_interpretation_threshold_salience:\n            rospy.loginfo(f\"{self.node_name}: Triggering LLM for sensory interpretation (Modality: {modality}, Salience: {self.cumulative_sensory_salience:.2f}).\")\n            \n            context_for_llm = self._compile_llm_context_for_sensory_interpretation(raw_data)\n            llm_qualia_output = await self._interpret_sensory_data_llm(raw_data['raw_data_parsed'], modality, context_for_llm)\n\n            if llm_qualia_output:\n                qualia_type = llm_qualia_output.get('qualia_type', 'unspecified_perception')\n                description_summary = llm_qualia_output.get('description_summary', 'No summary.')\n                salience_score = max(0.0, min(1.0, llm_qualia_output.get('salience_score', 0.0)))\n                llm_interpretation_notes = llm_qualia_output.get('llm_interpretation_notes', 'LLM interpreted sensory data.')\n                rospy.loginfo(f\"{self.node_name}: LLM Interpreted Qualia: '{description_summary}' (Salience: {salience_score:.2f}).\")\n            else:\n                rospy.logwarn(f\"{self.node_name}: LLM sensory interpretation failed. Applying simple fallback.\")\n                qualia_type, description_summary, salience_score = self._apply_simple_sensory_rules(raw_data)\n                llm_interpretation_notes = \"Fallback to simple rules due to LLM failure.\"\n        else:\n            rospy.logdebug(f\"{self.node_name}: Insufficient cumulative salience ({self.cumulative_sensory_salience:.2f}) for LLM sensory interpretation. Applying simple rules.\")\n            qualia_type, description_summary, salience_score = self._apply_simple_sensory_rules(raw_data)\n            llm_interpretation_notes = \"Fallback to simple rules due to low salience.\"\n\n        # Update salience_score based on the raw data's urgency if it was not high enough for LLM\n        # This ensures high urgency raw data still gets a decent salience_score even without LLM\n        salience_score = max(salience_score, raw_data.get('urgency', 0.0))\n\n        self.save_sensory_log(\n            id=qualia_id,\n            timestamp=timestamp,\n            qualia_type=qualia_type,\n            modality=modality,\n            description_summary=description_summary,\n            salience_score=salience_score,\n            llm_interpretation_notes=llm_interpretation_notes,\n            raw_data_hash=raw_data_hash,\n            context_snapshot_json=json.dumps(self._compile_llm_context_for_sensory_interpretation(raw_data))\n        )\n        self.publish_sensory_qualia(\n            timestamp=timestamp,\n            qualia_id=qualia_id,\n            qualia_type=qualia_type,\n            modality=modality,\n            description_summary=description_summary,\n            salience_score=salience_score,\n            raw_data_hash=raw_data_hash\n        )\n        self.cumulative_sensory_salience = 0.0 # Reset after processing\n\n    async def _interpret_sensory_data_llm(self, raw_data_parsed, modality, context_for_llm):\n        \"\"\"\n        Uses the LLM to interpret raw sensor data into meaningful sensory qualia.\n        \"\"\"\n        prompt_text = f\"\"\"\n        You are the Sensory Qualia Module of a robot's cognitive architecture, powered by a large language model. Your role is to interpret `raw_sensor_data` from a specific `modality` into high-level `SensoryQualia`. This involves describing the perception, categorizing its `qualia_type`, and assigning a `salience_score` based on its importance and current `cognitive_context`.\n\n        Raw Sensor Data:\n        --- Raw Data (from {modality} sensor) ---\n        {json.dumps(raw_data_parsed, indent=2)}\n\n        Robot's Current Cognitive Context (for interpreting sensory data):\n        --- Cognitive Context ---\n        {json.dumps(context_for_llm, indent=2)}\n\n        Based on this, provide:\n        1.  `qualia_type`: string (The type of perception, e.g., 'visual_perception', 'auditory_stimulus', 'tactile_sensation', 'temperature_change', 'proximity_detection').\n        2.  `description_summary`: string (A concise, human-readable summary of the sensory experience, e.g., \"Detected a human figure approaching\", \"Heard a loud bang\", \"Felt a warm surface\").\n        3.  `salience_score`: number (0.0 to 1.0, indicating how attention-grabbing or important this sensory input is. Higher score for unexpected, urgent, or goal-relevant perceptions.)\n        4.  `llm_interpretation_notes`: string (Brief notes on your interpretation process and why certain details were highlighted.)\n\n        Consider:\n        -   **Raw Data**: What are the key features in the raw data? (e.g., for camera: presence of objects, colors, movement; for audio: loudness, frequency, speech; for lidar: distance, obstacles).\n        -   **Modality**: How does the modality influence interpretation?\n        -   **Cognitive Directives**: Was there a directive to `FocusSensoryProcessing` on something specific (e.g., \"look for red objects\")?\n        -   **Attention State**: What is the robot's current `focus_target` and `priority_score`? Does this sensory input align with it, increasing its salience?\n        -   **World Model State**: Does this sensory data confirm or contradict the current `world_model_state`? Is it a `significant_change_flag`?\n\n        Your response must be in JSON format, containing:\n        1.  'timestamp': string (current ROS time)\n        2.  'qualia_type': string\n        3.  'description_summary': string\n        4.  'salience_score': number\n        5.  'llm_interpretation_notes': string\n        \"\"\"\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"timestamp\": {\"type\": \"string\"},\n                \"qualia_type\": {\"type\": \"string\"},\n                \"description_summary\": {\"type\": \"string\"},\n                \"salience_score\": {\"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0},\n                \"llm_interpretation_notes\": {\"type\": \"string\"}\n            },\n            \"required\": [\"timestamp\", \"qualia_type\", \"description_summary\", \"salience_score\", \"llm_interpretation_notes\"]\n        }\n\n        llm_output_str = await self._call_llm_api(prompt_text, response_schema, temperature=0.3, max_tokens=250)\n\n        if not llm_output_str.startswith(\"Error:\"):\n            try:\n                llm_data = json.loads(llm_output_str)\n                # Ensure numerical fields are floats\n                if 'salience_score' in llm_data: llm_data['salience_score'] = float(llm_data['salience_score'])\n                return llm_data\n            except json.JSONDecodeError as e:\n                self._report_error(\"LLM_PARSE_ERROR\", f\"Failed to parse LLM response for sensory qualia: {e}. Raw: {llm_output_str}\", 0.8)\n                return None\n        else:\n            self._report_error(\"LLM_SENSORY_INTERPRETATION_FAILED\", f\"LLM call failed for sensory interpretation: {llm_output_str}\", 0.9)\n            return None\n\n    def _apply_simple_sensory_rules(self, raw_data):\n        \"\"\"\n        Fallback mechanism to process raw sensor data into simple qualia using rule-based logic\n        if LLM is not triggered or fails.\n        \"\"\"\n        modality = raw_data.get('modality', 'unknown')\n        raw_data_parsed = raw_data.get('raw_data_parsed', {})\n        urgency = raw_data.get('urgency', 0.0)\n        \n        qualia_type = 'generic_perception'\n        description_summary = f\"Processed raw data from {modality} sensor.\"\n        salience_score = urgency * 0.5 # Base salience on urgency\n\n        # Example simple rules based on modality and hypothetical content\n        if modality == 'camera':\n            if 'object_detected' in raw_data_parsed and raw_data_parsed['object_detected']:\n                qualia_type = 'visual_object_detection'\n                description_summary = f\"Visually detected: {raw_data_parsed.get('object_type', 'an object')}.\"\n                salience_score = max(salience_score, 0.4)\n            elif 'motion_detected' in raw_data_parsed and raw_data_parsed['motion_detected']:\n                qualia_type = 'visual_motion'\n                description_summary = \"Detected visual motion.\"\n                salience_score = max(salience_score, 0.3)\n        elif modality == 'microphone':\n            if 'sound_level' in raw_data_parsed and raw_data_parsed['sound_level'] > 70: # dB threshold\n                qualia_type = 'auditory_loud_sound'\n                description_summary = \"Heard a loud sound.\"\n                salience_score = max(salience_score, 0.6)\n            elif 'speech_detected' in raw_data_parsed and raw_data_parsed['speech_detected']:\n                qualia_type = 'auditory_speech'\n                description_summary = \"Detected human speech.\"\n                salience_score = max(salience_score, 0.5)\n        elif modality == 'lidar':\n            if 'closest_distance' in raw_data_parsed and raw_data_parsed['closest_distance'] < 0.5: # meters\n                qualia_type = 'proximity_alert'\n                description_summary = \"Obstacle detected very close.\"\n                salience_score = max(salience_score, 0.8)\n\n        rospy.logwarn(f\"{self.node_name}: Simple rule: Generated fallback qualia for '{modality}'. Summary: {description_summary}.\")\n        return qualia_type, description_summary, salience_score\n\n\n    def _compile_llm_context_for_sensory_interpretation(self, raw_data):\n        \"\"\"\n        Gathers and formats all relevant cognitive state data for the LLM's\n        sensory interpretation.\n        \"\"\"\n        context = {\n            \"current_time\": rospy.get_time(),\n            \"raw_sensor_data_source\": {\n                \"timestamp\": raw_data.get('timestamp'),\n                \"sensor_id\": raw_data.get('sensor_id'),\n                \"modality\": raw_data.get('modality'),\n                \"urgency_from_sensor\": raw_data.get('urgency', 0.0)\n            },\n            \"recent_cognitive_inputs\": {\n                \"attention_state\": self.recent_attention_states[-1] if self.recent_attention_states else \"N/A\",\n                \"world_model_state\": self.recent_world_model_states[-1] if self.recent_world_model_states else \"N/A\",\n                \"cognitive_directives_for_self\": [d for d in self.recent_cognitive_directives if d.get('target_node') == self.node_name]\n            }\n        }\n        \n        # Deep parse any nested JSON strings in context for better LLM understanding\n        for category_key in context[\"recent_cognitive_inputs\"]:\n            item = context[\"recent_cognitive_inputs\"][category_key]\n            if isinstance(item, dict):\n                for field, value in item.items():\n                    if isinstance(value, str) and field.endswith('_json'):\n                        try: item[field] = json.loads(value)\n                        except json.JSONDecodeError: pass\n\n        return context\n\n    # --- Database and Publishing Functions ---\n    def save_sensory_log(self, id, timestamp, qualia_type, modality, description_summary, salience_score, llm_interpretation_notes, raw_data_hash, context_snapshot_json):\n        \"\"\"Saves a sensory qualia entry to the SQLite database.\"\"\"\n        try:\n            self.cursor.execute('''\n                INSERT INTO sensory_log (id, timestamp, qualia_type, modality, description_summary, salience_score, llm_interpretation_notes, raw_data_hash, context_snapshot_json)\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n            ''', (id, timestamp, qualia_type, modality, description_summary, salience_score, llm_interpretation_notes, raw_data_hash, context_snapshot_json))\n            self.conn.commit()\n            rospy.logdebug(f\"{self.node_name}: Saved sensory log (ID: {id}, Type: {qualia_type}).\")\n        except sqlite3.Error as e:\n            self._report_error(\"DB_SAVE_ERROR\", f\"Failed to save sensory log: {e}\", 0.9)\n        except Exception as e:\n            self._report_error(\"UNEXPECTED_SAVE_ERROR\", f\"Unexpected error in save_sensory_log: {e}\", 0.9)\n\n\n    def publish_sensory_qualia(self, timestamp, qualia_id, qualia_type, modality, description_summary, salience_score, raw_data_hash):\n        \"\"\"Publishes the processed sensory qualia.\"\"\"\n        try:\n            if isinstance(SensoryQualia, type(String)): # Fallback to String message\n                qualia_data = {\n                    'timestamp': timestamp,\n                    'qualia_id': qualia_id,\n                    'qualia_type': qualia_type,\n                    'modality': modality,\n                    'description_summary': description_summary,\n                    'salience_score': salience_score,\n                    'raw_data_hash': raw_data_hash\n                }\n                self.pub_sensory_qualia.publish(json.dumps(qualia_data))\n            else:\n                qualia_msg = SensoryQualia()\n                qualia_msg.timestamp = timestamp\n                qualia_msg.qualia_id = qualia_id\n                qualia_msg.qualia_type = qualia_type\n                qualia_msg.modality = modality\n                qualia_msg.description_summary = description_summary\n                qualia_msg.salience_score = salience_score\n                qualia_msg.raw_data_hash = raw_data_hash\n                self.pub_sensory_qualia.publish(qualia_msg)\n\n            rospy.loginfo(f\"{self.node_name}: Published Sensory Qualia. Type: '{qualia_type}', Summary: '{description_summary}'.\")\n\n        except Exception as e:\n            self._report_error(\"PUBLISH_SENSORY_QUALIA_ERROR\", f\"Failed to publish sensory qualia for '{modality}': {e}\", 0.7)\n\n    def publish_cognitive_directive(self, directive_type, target_node, command_payload, urgency, reason=\"\"):\n        \"\"\"Helper to publish a CognitiveDirective message.\"\"\"\n        timestamp = str(rospy.get_time())\n        try:\n            if isinstance(CognitiveDirective, type(String)): # Fallback to String message\n                directive_data = {\n                    'timestamp': timestamp,\n                    'directive_type': directive_type,\n                    'target_node': target_node,\n                    'command_payload': command_payload, # Already JSON string\n                    'urgency': urgency,\n                    'reason': reason\n                }\n                self.pub_cognitive_directive.publish(json.dumps(directive_data))\n            else:\n                directive_msg = CognitiveDirective()\n                directive_msg.timestamp = timestamp\n                directive_msg.directive_type = directive_type\n                directive_msg.target_node = target_node\n                directive_msg.command_payload = command_payload\n                directive_msg.urgency = urgency\n                directive_msg.reason = reason\n                self.pub_cognitive_directive.publish(directive_msg)\n            rospy.logdebug(f\"{self.node_name}: Issued Cognitive Directive '{directive_type}' to '{target_node}'.\")\n        except Exception as e:\n            rospy.logerr(f\"{self.node_name}: Failed to issue cognitive directive from Sensory Qualia Node: {e}\")\n\n\n    def run(self):\n        \"\"\"Starts the ROS node and keeps it spinning.\"\"\"\n        rospy.spin()\n\n    def __del__(self):\n        \"\"\"Ensures the database connection is closed on node shutdown and async loop is stopped.\"\"\"\n        rospy.loginfo(f\"{self.node_name} shutting down. Closing database connection and asyncio loop.\")\n        if hasattr(self, 'conn') and self.conn:\n            self.conn.close()\n        self._shutdown_async_loop()\n\nif __name__ == '__main__':\n    try:\n        node = SensoryQualiaNode()\n        node.run()\n    except rospy.ROSInterruptException:\n        rospy.loginfo(f\"{rospy.get_name()} interrupted by ROS shutdown.\")\n        if 'node' in locals() and isinstance(node, SensoryQualiaNode):\n            node._shutdown_async_loop()\n            if hasattr(node, 'conn'): node.conn.close()\n    except Exception as e:\n        rospy.logerr(f\"{rospy.get_name()} encountered an unexpected error: {e}\")\n        if 'node' in locals() and isinstance(node, SensoryQualiaNode):\n            node._shutdown_async_loop()\n            if hasattr(node, 'conn'): node.conn.close()","outputs":[],"execution_count":null,"metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}